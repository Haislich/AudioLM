{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZV3wbF6X_t63",
        "outputId": "31d55e11-3aa4-47cc-8eea-5e9b8cd65e10"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "afIICT1doURj"
      },
      "source": [
        "# Data preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xTOmq99goYTA",
        "outputId": "47bb48bd-9c97-426b-f467-b86cb8a76eb9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "fatal: destination path 'libri-light' already exists and is not an empty directory.\n",
            "fatal: destination path 'AudioLM' already exists and is not an empty directory.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/facebookresearch/libri-light.git\n",
        "!git clone https://github.com/Haislich/AudioLM.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "louGw6iIuOGn"
      },
      "outputs": [],
      "source": [
        "#!python /content/libri-light/data_preparation/build_all_stats.py /content/drive/MyDrive/AudioLMDataset/datasets_raw/small/small /content/drive/MyDrive/AudioLMDataset/ecciu"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ZJLU4eMXFh2"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "mO0-NPUUsp45"
      },
      "outputs": [],
      "source": [
        "#!pip install datasets\n",
        "!pip install librosa  ##Nuova dipendenza, devo aggiungerla\n",
        "import os\n",
        "import numpy as np\n",
        "from transformers import Wav2Vec2BertModel, AutoProcessor\n",
        "import torch\n",
        "from torch.nn.utils.rnn import pad_sequence     ##Nuova dipendenza, devo aggiungerla\n",
        "from torch.utils.data import Dataset, DataLoader ##Nuova dipendenza, devo aggiungerla\n",
        "import random as rd\n",
        "from tqdm import tqdm\n",
        "import librosa as lb\n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import StandardScaler"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_di1MNzmW5DG"
      },
      "source": [
        "# Classes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mzItcArHW8d0"
      },
      "source": [
        "## Utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "S-YGmHvOW7cu"
      },
      "outputs": [],
      "source": [
        "def set_seed(seed):\n",
        "    rd.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "\n",
        "def count_flac(data_path):\n",
        "  data_list = os.walk(data_path)\n",
        "  cnt=0\n",
        "\n",
        "  for dirpath, dirnames, filenames in data_list:\n",
        "    for filename in filenames:\n",
        "      path_to_audio = os.path.join(dirpath, filename)\n",
        "      if path_to_audio.endswith(\".flac\"):\n",
        "        cnt+=1\n",
        "\n",
        "  return cnt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k2sZP4vrQKYA"
      },
      "source": [
        "## Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ejBjUPqXHp5X"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q8JY9tA0Guqt",
        "outputId": "ac330da2-e670-462a-f5f1-9f188ebaea19"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "Wav2Vec2BertModel(\n",
              "  (feature_projection): Wav2Vec2BertFeatureProjection(\n",
              "    (layer_norm): LayerNorm((160,), eps=1e-05, elementwise_affine=True)\n",
              "    (projection): Linear(in_features=160, out_features=1024, bias=True)\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (encoder): Wav2Vec2BertEncoder(\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "    (layers): ModuleList(\n",
              "      (0-23): 24 x Wav2Vec2BertEncoderLayer(\n",
              "        (ffn1_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (ffn1): Wav2Vec2BertFeedForward(\n",
              "          (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (intermediate_dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (intermediate_act_fn): SiLU()\n",
              "          (output_dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (output_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (self_attn_dropout): Dropout(p=0.0, inplace=False)\n",
              "        (self_attn): Wav2Vec2BertSelfAttention(\n",
              "          (linear_q): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          (linear_k): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          (linear_v): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          (linear_out): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "          (distance_embedding): Embedding(73, 64)\n",
              "        )\n",
              "        (conv_module): Wav2Vec2BertConvolutionModule(\n",
              "          (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (pointwise_conv1): Conv1d(1024, 2048, kernel_size=(1,), stride=(1,), bias=False)\n",
              "          (glu): GLU(dim=1)\n",
              "          (depthwise_conv): Conv1d(1024, 1024, kernel_size=(31,), stride=(1,), groups=1024, bias=False)\n",
              "          (depthwise_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (activation): SiLU()\n",
              "          (pointwise_conv2): Conv1d(1024, 1024, kernel_size=(1,), stride=(1,), bias=False)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ffn2_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (ffn2): Wav2Vec2BertFeedForward(\n",
              "          (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (intermediate_dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (intermediate_act_fn): SiLU()\n",
              "          (output_dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (output_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (adapter): Wav2Vec2BertAdapter(\n",
              "    (layers): ModuleList(\n",
              "      (0): Wav2Vec2BertAdapterLayer(\n",
              "        (residual_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (residual_conv): Conv1d(1024, 2048, kernel_size=(3,), stride=(2,), padding=(1,))\n",
              "        (activation): GLU(dim=1)\n",
              "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (self_attn_conv): Conv1d(1024, 2048, kernel_size=(3,), stride=(2,), padding=(1,))\n",
              "        (self_attn): Wav2Vec2BertSelfAttention(\n",
              "          (linear_q): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          (linear_k): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          (linear_v): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          (linear_out): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (self_attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "        (ffn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (ffn): Wav2Vec2BertFeedForward(\n",
              "          (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (intermediate_dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (intermediate_act_fn): ReLU()\n",
              "          (output_dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (output_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "set_seed(42)\n",
        "#from datasets import load_dataset\n",
        "\n",
        "data_path = \"/content/drive/MyDrive/AudioLMDataset/dataset_segmented\"\n",
        "#dataset = load_dataset(\"hf-internal-testing/librispeech_asr_demo\", \"clean\", split=\"validation\")\n",
        "processor = AutoProcessor.from_pretrained(\"hf-audio/wav2vec2-bert-CV16-en\")\n",
        "model = Wav2Vec2BertModel.from_pretrained(\"hf-audio/wav2vec2-bert-CV16-en\")\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "K7IOpGGbMS40"
      },
      "outputs": [],
      "source": [
        "class AudioDataset(Dataset):\n",
        "  def __init__(self, path, processor, sr=16000):\n",
        "    self.path = path\n",
        "    self.processor = processor\n",
        "    self.sr = sr\n",
        "    self.audios = self.collate_audios(path)\n",
        "    self.num_audio = self.__len__(),\n",
        "    self.processor_dim = 2999,\n",
        "    self.target_length = sr*60\n",
        "\n",
        "  def collate_audios(self, path):\n",
        "    path_audios = []\n",
        "    for dirpath, _, filenames in os.walk(path):\n",
        "      for filename in filenames:\n",
        "        path_to_audio = os.path.join(dirpath, filename)\n",
        "        if path_to_audio.endswith(\".flac\"):\n",
        "          path_audios.append(path_to_audio)\n",
        "\n",
        "    return path_audios\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.audios)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    path = self.audios[idx]\n",
        "    audio, _ = lb.load(path, sr=self.sr)\n",
        "    if(len(audio)) > self.target_length:\n",
        "      audio = audio[:self.target_length]\n",
        "\n",
        "    input = processor(\n",
        "        audio=audio,\n",
        "        return_tensors=\"pt\",\n",
        "        padding=True,\n",
        "        sampling_rate=16000\n",
        "      )\n",
        "    len_tens = input['input_features'].shape[1]\n",
        "    if len_tens < self.processor_dim[0]:\n",
        "      #print(input['input_features'].shape)\n",
        "      paddin_len = self.processor_dim[0]-len_tens\n",
        "      padding = (0, 0, 0, self.processor_dim[0]-len_tens, 0, 0)\n",
        "      input['input_features'] = pad(input['input_features'], padding, \"constant\", value=0)\n",
        "      padding_mask = (0, paddin_len)\n",
        "      input['attention_mask'] = pad(input['attention_mask'], padding_mask, \"constant\", value=0)\n",
        "\n",
        "    return input\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "HzzN6ZLhTtBj"
      },
      "outputs": [],
      "source": [
        "def collate_fn(batch):\n",
        "  input_features = [input['input_features'] for input in batch]\n",
        "  attention_masks = [input['attention_mask'] for input in batch]\n",
        "  #print(\"Output prima torch.stack: \", [input.shape for input in input_features])\n",
        "  #print(\"Output prima torch.stack: \", [input.shape for input in attention_masks])\n",
        "\n",
        "  input = torch.cat(input_features, dim=0)\n",
        "  attention_mask = torch.cat(attention_masks, dim=0)\n",
        "  #print(\"Output dopo torch.stack: \", input.shape)\n",
        "  #print(\"Output dopo torch.stack: \", attention_mask.shape)\n",
        "\n",
        "  return {'input_features': input, 'attention_mask': attention_mask}\n",
        "\n",
        "batch_size = 1\n",
        "dataset_loader = AudioDataset(data_path, processor)\n",
        "dataset = DataLoader(dataset_loader, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "TCh_U_jeEduq"
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "from time import sleep"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "DG29kWALHReG"
      },
      "outputs": [],
      "source": [
        "def from_audio_2_embeddings(dataset, model, dataset_segmented=True, max_files=None):\n",
        "\n",
        "    model.eval()\n",
        "    total_batches = len(dataset)\n",
        "\n",
        "    audio_embeddings = []\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "    sleep(3)\n",
        "\n",
        "    pbar = tqdm(total=min(total_batches, max_files) if max_files is not None else total_batches, desc=\"Featuring audios...\")\n",
        "\n",
        "\n",
        "    with torch.no_grad():\n",
        "      for i, batch in enumerate(dataset):\n",
        "        #print(batch['input_features'])\n",
        "        #print(len(batch['attention_mask']))\n",
        "        if max_files is not None and i >= max_files:\n",
        "            break\n",
        "        input_features = batch['input_features'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        output = model(input_features, attention_mask=attention_mask, output_hidden_states=True, return_dict=True)\n",
        "        seventh_layer_output = output.hidden_states[6].detach().cpu().numpy()\n",
        "        audio_embeddings.append(seventh_layer_output)\n",
        "        pbar.update(1)\n",
        "        #torch.cuda.empty_cache()\n",
        "        #gc.collect()\n",
        "\n",
        "    pbar.close()\n",
        "\n",
        "    audio_embeddings = np.concatenate(audio_embeddings, axis=0)\n",
        "\n",
        "    return audio_embeddings\n",
        "\n",
        "\n",
        "\n",
        "def from_embd_to_semToken(audio_embeddings):\n",
        "    scaler = StandardScaler()\n",
        "    reshaped_data = audio_embeddings.reshape(-1, audio_embeddings.shape[2])\n",
        "\n",
        "    audio_embeddings = scaler.fit_transform(reshaped_data)\n",
        "\n",
        "    k_means = KMeans(n_clusters=1024, random_state=42)\n",
        "    k_means.fit(audio_embeddings)\n",
        "\n",
        "    return k_means.labels_\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7z_Rmpap9qBG",
        "outputId": "20e2f713-4d2d-434a-ce93-7e5cb62d60ba"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Featuring audios...: 100%|██████████| 15/15 [00:38<00:00,  2.55s/it]\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "audio_embed = from_audio_2_embeddings(dataset, model, True, 15)\n",
        "audio_embed.shape\n",
        "semantic_tokens = from_embd_to_semToken(audio_embed)\n",
        "#audio_embed = test_function(data_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EZRvtu8-8cxi",
        "outputId": "350d4d11-56d3-44ab-99cf-8a80fc7fc92a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "145"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.cuda.empty_cache()\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wa46cIKt_1dJ"
      },
      "outputs": [],
      "source": [
        "#!pip install nvitop\n",
        "!nvitop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q21ToY-h3Ht5"
      },
      "outputs": [],
      "source": [
        "print(torch.cuda.memory_summary(device=None, abbreviated=False))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C-Ws9j2LG4Ry"
      },
      "source": [
        "# Transformer class"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
