{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if running locally or on colab\n",
    "try:\n",
    "    import google.colab # type: ignore  # noqa: F401\n",
    "    # Check if running locally or in colab\n",
    "    %pip install https://github.com/Haislich/AudioLM/raw/main/dist/audiolm-0.1.0.tar.gz\n",
    "    # Thanks stack overflow https://stackoverflow.com/questions/77451004/attributeerror-module-numpy-linalg-umath-linalg-has-no-attribute-ilp64#:~:text=pip%20install%20numpy%3D%3D1.23.5\n",
    "    %pip install numpy==1.23.5\n",
    "except:  # noqa: E722\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## AudioLM: A Language Modeling Approach to Audio Generation\n",
    "\n",
    "**AudioLM** represents a self-supervised learning methodology aimed at generating high-quality audio with sustained long-term consistency. This technique maps input audio to a sequence of discrete tokens, treating the audio generation process akin to a language modeling task within this representational framework. The proposal introduces a hybrid tokenization scheme, employing the discretized activations of a pre-trained masked language model on audio to capture long-term structures, and discrete codes from a neural audio codec to ensure high-fidelity synthesis.\n",
    "\n",
    "#### Tokenization Approach:\n",
    "The core innovation of this method lies in the **Hybrid Tokenization Scheme**, crucial for subsequent conditioning of transformers.\n",
    "\n",
    "**Hybrid Tokenization Scheme**: AudioLM combines semantic and acoustic tokens hierarchically to strike a balance between long-term consistency and high-quality audio synthesis. The semantic tokens are extracted from a pre-trained w2v-BERT model, while the acoustic tokens are derived from a finely-tuned SoundStream on a speech dataset. This approach emphasizes their complementary strengths in phonetic discriminability and reconstruction quality.\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"images/semantic_acoustic_tokens.png\" alt=\"Hybrid Tokenization Scheme\" width=\"400\">\n",
    "</div>\n",
    "\n",
    "#### Hybrid Tokenization:\n",
    "As depicted in the image, the hybrid tokenization scheme is divided into two parallel components:\n",
    "1. **Semantic Tokens: W2V-Bert-based tokenizer**\n",
    "   This component is responsible for extracting features from audio waveforms as a 1024-dimensional embedding. A K-means quantizer with 1024 clusters discretizes these embeddings. Each feature vector in the space is associated with a reference cluster based on its proximity, thus identifying the i-th **semantic token**.\n",
    "\n",
    "2. **Acoustic Tokens: Soundstream tokenizer**\n",
    "    .....\n",
    "\n",
    "#### Autoregressive Prediction:\n",
    "The prediction phase is subsequently divided into three distinct stages:\n",
    "1. **Semantic Stage:** This initial stage utilizes semantic tokenization to train a decoder-only transformer. By receiving a sequence of semantic tokens, it performs an autoregressive prediction task during inference. Specifically, the first stage models $$p(z_t|z_{t-1})$$, focusing on the autoregressive prediction of semantic tokens to capture long-term temporal structure.\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"images/semantic_stage.png\" alt=\"Semantic Stage\" width=\"400\">\n",
    "</div>\n",
    "\n",
    "2. **Coarse Acoustic Stage:** Similar to the first, this stage performs an autoregressive prediction task; however, unlike the first, the second transformer receives a sequence of acoustic tokens input from the first four quantizers of Soundstream, conditioned by the previously generated semantic tokens. Specifically, the acoustic tokens possess a hierarchical structure where coarse quantizers capture fundamental acoustic properties such as speaker identity and recording conditions. The second stage models $$p(y_t | z, y_{<t}, y_t)$$ for $q \\leq Q'$, representing the token sequence as $(z_1, z_2, \\ldots, z_{T_s}, y_1^{1}, y_1^{2}, \\ldots, y_{Q'}^{1}, y_2^{1}, \\ldots, y_{T_A}^{Q'})$, with $y_1$ being the first token predicted during training.\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"images/acoustic_stage.png\" alt=\"Coarse Acoustic Stage\" width=\"400\">\n",
    "</div>\n",
    "\n",
    "3. **Fine Acoustic Stage and Decoding:** Finally, this stage significantly improves audio quality by eliminating the lossy compression artifacts produced during the second stage. Here, the prompt and the sampled acoustic tokens are fed into the **SoundStream** decoder to reconstruct a waveform $\\hat x$.\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"images/decoding.png\" alt=\"Fine Acoustic Stage and Decoding\" width=\"400\">\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%load_ext autoreload\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from audiolm.data_preparation import AudioDataLoader\n",
    "from audiolm.w2v_hubert import W2VHuBert\n",
    "from audiolm.absolute_transformer import (\n",
    "    SemanticTransformer,\n",
    "    CoarseAcousticTransformer,\n",
    "    FineAcousticTransformer,\n",
    ")\n",
    "from audiolm.encodec import Encodec\n",
    "from audiolm.utils import (\n",
    "    get_latest_checkpoint_path,\n",
    "    get_model_path,\n",
    "    load_checkpoint,\n",
    "    load_model,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = Path(os.getcwd()) / Path(\"..\") / Path(\"data\") / Path(\"datasets\")\n",
    "SAVE_LOAD_PATH = Path(os.getcwd()) / Path(\"..\") / Path(\"data\")\n",
    "# DATA_PATH = Path(\"/Volumes/SSD-EXT/NN_project/dataset\")\n",
    "# SAVE_LOAD_PATH = Path(\"/Volumes/SSD-EXT/NN_project/models\")\n",
    "INTERVALS = 10\n",
    "EARLY_STOP_COUNTER = 0\n",
    "EARLY_STOPPING_RANGE = 5\n",
    "EPOCHS = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantic and Acoustic encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "semantic_encoder = W2VHuBert()\n",
    "acoustic_encoder_decoder = Encodec()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = AudioDataLoader(DATA_PATH / \"train\", batch_size=6,max_elems=35)\n",
    "val_dataloader = AudioDataLoader(DATA_PATH / \"val\", batch_size=6,max_elems=15)\n",
    "test_dataloader = AudioDataLoader(DATA_PATH / \"test\", batch_size=6,max_elems=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define what an abstract trainer should look like. This will give the skeleton for the specific trainers.\n",
    "This is needed because this framework uses a hierachical model approach, however at each level of the hierarchy only one transfomere is trained and much of the inherent logic is repeated.\n",
    "We can use this insight to create spoecialized classes for the training of a given transformer.\n",
    "The loss generator is the main component that will be changed the most during the specialization as it reflects how the input/output are created and is the only thing that changes between hierachies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainer class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import ceil\n",
    "from abc import ABC, abstractmethod\n",
    "from typing import Optional\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "from audiolm.encodec import Encodec\n",
    "from audiolm.constants import DEVICE\n",
    "from audiolm.data_preparation import AudioDataLoader\n",
    "from audiolm.w2v_hubert import W2VHuBert\n",
    "from audiolm.utils import save_checkpoint, save_model\n",
    "\n",
    "class Trainer(ABC):\n",
    "    \"\"\"\n",
    "    Trainer class for training a Transformer model.\n",
    "    \"\"\"\n",
    "\n",
    "    @abstractmethod\n",
    "    # pylint: disable =too-many-arguments\n",
    "    def __init__(\n",
    "        self,\n",
    "        semantic_encoder: Optional[W2VHuBert] = None,\n",
    "        semantic_transformer: Optional[SemanticTransformer] = None,\n",
    "        acoustic_encoder_decoder: Optional[Encodec] = None,\n",
    "        coarse_acoustic_transformer: Optional[CoarseAcousticTransformer] = None,\n",
    "        fine_acoustic_transformer: Optional[FineAcousticTransformer] = None,\n",
    "        train_dataloader: Optional[AudioDataLoader] = None,\n",
    "        val_dataloader: Optional[AudioDataLoader] = None,\n",
    "        test_dataloader: Optional[AudioDataLoader] = None,\n",
    "        loss: Optional[nn.Module] = None,\n",
    "        optimizer: Optional[torch.optim.Optimizer] = None,\n",
    "        intervals: Optional[int] = None,\n",
    "        save_path: Optional[os.PathLike] = None,\n",
    "        early_stop_counter: Optional[int] = None,\n",
    "        early_stopping_range: Optional[int] = None,\n",
    "        epochs: Optional[int] = None,\n",
    "    ):\n",
    "        self.semantic_encoder = semantic_encoder\n",
    "        self.semantic_transformer = semantic_transformer\n",
    "        self.acoustic_encoder_decoder = acoustic_encoder_decoder\n",
    "        self.coarse_acoustic_transformer = coarse_acoustic_transformer\n",
    "        self.fine_acoustic_transformer = fine_acoustic_transformer\n",
    "        self.train_dataloader = train_dataloader\n",
    "        self.val_dataloader = val_dataloader\n",
    "        self.test_dataloader = test_dataloader\n",
    "        self.optimizer = optimizer\n",
    "        self.intervals = intervals\n",
    "        self.epochs = epochs\n",
    "        self.save_path = save_path\n",
    "        self.best_val_loss = float(\"inf\")\n",
    "        self.early_stopping_range = early_stopping_range\n",
    "        self.early_stop_counter = early_stop_counter\n",
    "        self.loss = loss\n",
    "        if save_path is not None and not os.path.exists(self.save_path):\n",
    "            os.makedirs(self.save_path)\n",
    "\n",
    "    # region: Abstract methods, this methods must be redefined accordingly.\n",
    "    @abstractmethod\n",
    "    def loss_generator(self, batch):\n",
    "        \"\"\"Generate loss\"\"\"\n",
    "\n",
    "    @abstractmethod\n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Train the Transformer model.\n",
    "        \"\"\"\n",
    "\n",
    "    @abstractmethod\n",
    "    def test(self):\n",
    "        \"\"\"Test the model on the test dataset.\"\"\"\n",
    "\n",
    "    # endregion\n",
    "\n",
    "    # region: Private methods.\n",
    "    def _train_step(self, model: nn.Module) -> float:\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        \n",
    "        for batch in tqdm(self.train_dataloader, total = ceil(len(self.train_dataloader) / self.train_dataloader.batch_size) ):\n",
    "            batch = batch.to(DEVICE)\n",
    "            loss = self.loss_generator(batch)\n",
    "            train_loss += loss.item()\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "        train_loss /= len(self.train_dataloader)\n",
    "        return train_loss\n",
    "\n",
    "    def _validation_step(self, model: nn.Module) -> float:\n",
    "        model.eval()\n",
    "        validation_loss = 0\n",
    "        with torch.inference_mode():\n",
    "            for batch in self.val_dataloader:\n",
    "                batch = batch.to(DEVICE)\n",
    "                loss = self.loss_generator(batch)\n",
    "                validation_loss += loss.item()\n",
    "\n",
    "        validation_loss /= len(self.val_dataloader)\n",
    "\n",
    "        return validation_loss\n",
    "\n",
    "    def _train(self, model: nn.Module):\n",
    "        writer = SummaryWriter(Path(self.save_path) / \"runs\" / str(type(model).__name__))\n",
    "        for epoch in tqdm(range(self.epochs), total = self.epochs, desc=\"Training\"):\n",
    "            train_loss = self._train_step(model)\n",
    "            validation_loss = self._validation_step(model)\n",
    "            print(\"SAVING CHECKPOINT...\")\n",
    "            save_checkpoint(\n",
    "                model, epoch, self.optimizer, self.early_stop_counter, self.save_path\n",
    "            )\n",
    "            print(\"SAVING RUN FOR TENSORBOARD...\")\n",
    "            writer.add_scalars(\n",
    "                main_tag=f\"Loss_{str(type(model).__name__)}\",\n",
    "                tag_scalar_dict={\n",
    "                    \"train_loss\": train_loss,\n",
    "                    \"validation_loss\": validation_loss,\n",
    "                },\n",
    "                global_step=epoch,\n",
    "            )\n",
    "\n",
    "            if validation_loss < self.best_val_loss:\n",
    "                self.best_val_loss = validation_loss\n",
    "                self.early_stop_counter = 0\n",
    "            else:\n",
    "                self.early_stop_counter += 1\n",
    "\n",
    "            if self.early_stop_counter >= self.early_stopping_range:\n",
    "                print(f\"Early stopping training at epoch: {epoch+1}\")\n",
    "                break\n",
    "        writer.flush()\n",
    "        writer.close()\n",
    "        save_model(model, self.save_path)\n",
    "\n",
    "    def _test(self, model):\n",
    "        model.eval()\n",
    "        test_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(self.test_dataloader, desc=\"Testing\"):\n",
    "                batch = batch.to(DEVICE)\n",
    "                loss = self.loss_generator(batch)\n",
    "                test_loss += loss.item()\n",
    "\n",
    "        test_loss /= len(self.test_dataloader)\n",
    "        print(f\"Test Loss: {test_loss: .4f}\")\n",
    "\n",
    "        return test_loss\n",
    "    # endregion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantic Trainer class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SemanticTrainer(Trainer):\n",
    "    \"\"\"Trainer class derived from `Trainer`.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        semantic_encoder: W2VHuBert,\n",
    "        semantic_transformer: SemanticTransformer,\n",
    "        train_dataloader: AudioDataLoader,\n",
    "        val_dataloader: AudioDataLoader,\n",
    "        test_dataloader: AudioDataLoader,\n",
    "        loss: nn.Module,\n",
    "        optimizer: torch.optim.Optimizer,\n",
    "        intervals: int,\n",
    "        save_path: Path,\n",
    "        early_stop_counter: int,\n",
    "        early_stopping_range: int,\n",
    "        epochs: int,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Takes as input `semantic_encoder` and `semantic_transformer`.\n",
    "        They determine the `semantic_modelling`.\n",
    "\n",
    "        `semantic_encoder` must be trained ahead of time, this trainer only\n",
    "        trains `semantic_transformer`.\n",
    "\n",
    "        Args\n",
    "        ----\n",
    "            `semantic_encoder` (W2VHuBert)\n",
    "\n",
    "            `semantic_transformer` (TransformerDecoderOnly)\n",
    "\n",
    "            `train_dataloader` (AudioDataLoader)\n",
    "\n",
    "            `val_dataloader` (AudioDataLoader)\n",
    "\n",
    "            `test_dataloader` (AudioDataLoader)\n",
    "\n",
    "            `loss` (nn.Module)\n",
    "\n",
    "            `optimizer` (torch.optim.Optimizer)\n",
    "\n",
    "            `intervals` (int)\n",
    "\n",
    "            `save_path` (Path)\n",
    "\n",
    "            `early_stop_counter` (int)\n",
    "\n",
    "            `early_stopping_range` (int)\n",
    "\n",
    "            `epochs` (int)\n",
    "        \"\"\"\n",
    "        super().__init__(\n",
    "            semantic_encoder=semantic_encoder,\n",
    "            semantic_transformer=semantic_transformer,\n",
    "            train_dataloader=train_dataloader,\n",
    "            val_dataloader=val_dataloader,\n",
    "            test_dataloader=test_dataloader,\n",
    "            loss=loss,\n",
    "            optimizer=optimizer,\n",
    "            intervals=intervals,\n",
    "            save_path=save_path,\n",
    "            early_stop_counter=early_stop_counter,\n",
    "            early_stopping_range=early_stopping_range,\n",
    "            epochs=epochs,\n",
    "        )\n",
    "        semantic_encoder = semantic_encoder.to(DEVICE)\n",
    "        semantic_transformer = semantic_transformer.to(DEVICE)\n",
    "\n",
    "    def loss_generator(self, batch):\n",
    "        semantic_encode = self.semantic_encoder(batch)\n",
    "\n",
    "        output, target = self.semantic_transformer.fit(semantic_encode)\n",
    "\n",
    "        loss = self.loss(output, target)\n",
    "        return loss\n",
    "\n",
    "    def train(self):\n",
    "        return self._train(self.semantic_transformer)\n",
    "\n",
    "    def test(self):\n",
    "        return self._test(self.semantic_transformer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_SEMANTIC = False\n",
    "\"\"\"Start training for the semantic model\"\"\"\n",
    "RESUME_SEMANTIC_TRAINING = True\n",
    "\"\"\"Start training from the an epoch\"\"\"\n",
    "\n",
    "semantic_transformer = SemanticTransformer(num_heads=4, layers=2)\n",
    "semantic_loss = nn.CrossEntropyLoss()\n",
    "semantic_optimizer = torch.optim.Adam(\n",
    "    semantic_transformer.parameters(), lr=0.001\n",
    ")\n",
    "\n",
    "semantic_transformer_root = (\n",
    "    Path(SAVE_LOAD_PATH)\n",
    "    / Path(\"models\")\n",
    "    / str(type(semantic_transformer).__name__)\n",
    ")\n",
    "semantic_transformer_path = get_model_path(semantic_transformer_root)\n",
    "checkpoint_path = get_latest_checkpoint_path(semantic_transformer_root)\n",
    "if not TRAIN_SEMANTIC and semantic_transformer_path:\n",
    "    load_model(semantic_transformer, semantic_transformer_path)\n",
    "else:\n",
    "    if RESUME_SEMANTIC_TRAINING and checkpoint_path:\n",
    "        print(\"Starting from the last epoch\")\n",
    "        semantic_transformer, _, semantic_optimizer, _ = load_checkpoint(\n",
    "            semantic_transformer, semantic_transformer_root\n",
    "        )\n",
    "    # elif not RESUME_SEMANTIC_TRAINING:\n",
    "    #     # Adapt to choose a given epoch\n",
    "    #     semantic_transformer, _, semantic_optimizer, _ = ...\n",
    "    semantic_trainer = SemanticTrainer(\n",
    "        semantic_encoder=semantic_encoder,\n",
    "        semantic_transformer=semantic_transformer,\n",
    "        train_dataloader=train_dataloader,\n",
    "        val_dataloader=val_dataloader,\n",
    "        test_dataloader=test_dataloader,\n",
    "        loss=semantic_loss,\n",
    "        optimizer=semantic_optimizer,\n",
    "        intervals=INTERVALS,\n",
    "        save_path=SAVE_LOAD_PATH,\n",
    "        early_stop_counter=EARLY_STOP_COUNTER,\n",
    "        early_stopping_range=EARLY_STOPPING_RANGE,\n",
    "        epochs=EPOCHS,\n",
    "    )\n",
    "    semantic_trainer.train()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir=../data/runs/Semantic_Transformer/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second stage of this hierachy is the coarse acoustic modelling ... informazioni sul coarse.\n",
    "come funziona la generazione della loss i.e come avviene il condizionamento ecc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coarse Acoustic Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoarseAcousticTrainer(Trainer):\n",
    "    \"\"\"Trainer class derived from `Trainer`.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        semantic_encoder: W2VHuBert,\n",
    "        semantic_transformer: SemanticTransformer,\n",
    "        acoustic_encoder_decoder: Encodec,\n",
    "        coarse_acoustic_transformer: CoarseAcousticTransformer,\n",
    "        train_dataloader: AudioDataLoader,\n",
    "        val_dataloader: AudioDataLoader,\n",
    "        test_dataloader: AudioDataLoader,\n",
    "        loss: nn.Module,\n",
    "        optimizer: torch.optim.Optimizer,\n",
    "        intervals: int,\n",
    "        save_path: Path,\n",
    "        early_stop_counter: int,\n",
    "        early_stopping_range: int,\n",
    "        generate_audio_len: int,\n",
    "        epochs: int,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Takes as input `semantic_encoder` and `semantic_transformer`.\n",
    "        They determine the `semantic_modelling`.\n",
    "\n",
    "        `semantic_encoder` must be trained ahead of time, this trainer only\n",
    "        trains `semantic_transformer`.\n",
    "\n",
    "        Args\n",
    "        ----\n",
    "            `semantic_encoder` (W2VHuBert)\n",
    "\n",
    "            `semantic_transformer` (TransformerDecoderOnly)\n",
    "\n",
    "            `train_dataloader` (AudioDataLoader)\n",
    "\n",
    "            `val_dataloader` (AudioDataLoader)\n",
    "\n",
    "            `test_dataloader` (AudioDataLoader)\n",
    "\n",
    "            `loss` (nn.Module)\n",
    "\n",
    "            `optimizer` (torch.optim.Optimizer)\n",
    "\n",
    "            `intervals` (int)\n",
    "\n",
    "            `save_path` (Path)\n",
    "\n",
    "            `early_stop_counter` (int)\n",
    "\n",
    "            `early_stopping_range` (int)\n",
    "\n",
    "            `epochs` (int)\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__(\n",
    "            semantic_encoder=semantic_encoder,\n",
    "            semantic_transformer=semantic_transformer,\n",
    "            acoustic_encoder_decoder=acoustic_encoder_decoder,\n",
    "            coarse_acoustic_transformer=coarse_acoustic_transformer,\n",
    "            train_dataloader=train_dataloader,\n",
    "            val_dataloader=val_dataloader,\n",
    "            test_dataloader=test_dataloader,\n",
    "            loss=loss,\n",
    "            optimizer=optimizer,\n",
    "            intervals=intervals,\n",
    "            save_path=save_path,\n",
    "            early_stop_counter=early_stop_counter,\n",
    "            early_stopping_range=early_stopping_range,\n",
    "            epochs=epochs,\n",
    "        )\n",
    "        semantic_encoder = semantic_encoder.to(DEVICE)\n",
    "        semantic_transformer = semantic_transformer.to(DEVICE)\n",
    "        acoustic_encoder_decoder = acoustic_encoder_decoder.to(DEVICE)\n",
    "        coarse_acoustic_transformer = coarse_acoustic_transformer.to(DEVICE)\n",
    "        self.generate_audio_len = generate_audio_len\n",
    "\n",
    "    def loss_generator(self, batch):\n",
    "\n",
    "        semantic_encode = self.semantic_encoder(batch)\n",
    "        semantic_token = self.semantic_transformer.generate(\n",
    "            semantic_encode, self.generate_audio_len * 50\n",
    "        )\n",
    "\n",
    "        coarse_acoustic_tokens, _, _ = self.acoustic_encoder_decoder.encode(batch)\n",
    "        conditioning = torch.cat((semantic_token, coarse_acoustic_tokens), dim=1)\n",
    "\n",
    "        output, target = self.coarse_acoustic_transformer.fit(conditioning)\n",
    "        loss = self.loss(output, target)\n",
    "        return loss\n",
    "\n",
    "    def train(self):\n",
    "        return self._train(self.coarse_acoustic_transformer)\n",
    "\n",
    "    def test(self):\n",
    "        return self._test(self.coarse_acoustic_transformer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_COARSE = False\n",
    "\"\"\"Start training for the semantic model\"\"\"\n",
    "RESUME_COARSE_TRAINING = True\n",
    "\"\"\"Start training from the an epoch\"\"\"\n",
    "GENERATE_AUDIO_LEN = 3\n",
    "\"\"\"Len in seconds of the audio generated\"\"\"\n",
    "\n",
    "semantic_transformer = SemanticTransformer(num_heads=4, layers=2)\n",
    "semantic_transformer_root = (\n",
    "    Path(SAVE_LOAD_PATH)\n",
    "    / Path(\"models\")\n",
    "    / str(type(semantic_transformer).__name__)\n",
    ")\n",
    "semantic_transformer_path = get_model_path(semantic_transformer_root)\n",
    "\n",
    "load_model(semantic_transformer, semantic_transformer_path)\n",
    "\n",
    "coarse_acoustic_transformer = CoarseAcousticTransformer()\n",
    "coarse_loss = nn.CrossEntropyLoss()\n",
    "coarse_optimizer = torch.optim.Adam(\n",
    "    coarse_acoustic_transformer.parameters(), lr=0.001\n",
    ")\n",
    "\n",
    "coarse_transformer_root = (\n",
    "    Path(SAVE_LOAD_PATH)\n",
    "    / Path(\"models\")\n",
    "    / str(type(coarse_acoustic_transformer).__name__)\n",
    ")\n",
    "coarse_transformer_path = get_model_path(coarse_transformer_root)\n",
    "checkpoint_path = get_latest_checkpoint_path(coarse_transformer_root)\n",
    "\n",
    "if not TRAIN_COARSE and coarse_transformer_path:\n",
    "    load_model(coarse_acoustic_transformer, coarse_transformer_path)\n",
    "else:\n",
    "    if RESUME_COARSE_TRAINING and checkpoint_path:\n",
    "        print(\"Starting from the last epoch\")\n",
    "        semantic_transformer, _, coarse_optimizer, _ = load_checkpoint(\n",
    "            semantic_transformer, coarse_transformer_root\n",
    "        )\n",
    "    # elif not RESUME_SEMANTIC_TRAINING:\n",
    "    #     # Adapt to choose a given epoch\n",
    "    #     semantic_transformer, _, semantic_optimizer, _ = ...\n",
    "    coarse_acoustic_trainer = CoarseAcousticTrainer(\n",
    "        semantic_encoder=semantic_encoder,\n",
    "        semantic_transformer=semantic_transformer,\n",
    "        acoustic_encoder_decoder=acoustic_encoder_decoder,\n",
    "        coarse_acoustic_transformer=coarse_acoustic_transformer,\n",
    "        train_dataloader=train_dataloader,\n",
    "        val_dataloader=val_dataloader,\n",
    "        test_dataloader=test_dataloader,\n",
    "        loss=coarse_loss,\n",
    "        optimizer=coarse_optimizer,\n",
    "        intervals=INTERVALS,\n",
    "        save_path=SAVE_LOAD_PATH,\n",
    "        early_stop_counter=EARLY_STOP_COUNTER,\n",
    "        early_stopping_range=EARLY_STOPPING_RANGE,\n",
    "        generate_audio_len=GENERATE_AUDIO_LEN,\n",
    "        epochs=EPOCHS,\n",
    "    )\n",
    "    coarse_acoustic_trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the sake of completness we implement the Fine acoustic trainer, even though it won't be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FineAcousticTrainer(Trainer):\n",
    "    \"\"\"Trainer class derived from `Trainer`.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        semantic_encoder: W2VHuBert,\n",
    "        semantic_transformer: SemanticTransformer,\n",
    "        acoustic_encoder_decoder: Encodec,\n",
    "        coarse_acoustic_transformer: CoarseAcousticTransformer,\n",
    "        fine_acoustic_transformer: FineAcousticTransformer,\n",
    "        train_dataloader: AudioDataLoader,\n",
    "        val_dataloader: AudioDataLoader,\n",
    "        test_dataloader: AudioDataLoader,\n",
    "        loss: nn.Module,\n",
    "        optimizer: torch.optim.Optimizer,\n",
    "        intervals: int,\n",
    "        save_path: Path,\n",
    "        early_stop_counter: int,\n",
    "        early_stopping_range: int,\n",
    "        generate_audio_len: int,\n",
    "        epochs: int,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Takes as input `semantic_encoder` and `semantic_transformer`.\n",
    "        They determine the `semantic_modelling`.\n",
    "\n",
    "        `semantic_encoder` must be trained ahead of time, this trainer only\n",
    "        trains `semantic_transformer`.\n",
    "\n",
    "        Args\n",
    "        ----\n",
    "            `semantic_encoder` (W2VHuBert)\n",
    "\n",
    "            `semantic_transformer` (TransformerDecoderOnly)\n",
    "\n",
    "            `train_dataloader` (AudioDataLoader)\n",
    "\n",
    "            `val_dataloader` (AudioDataLoader)\n",
    "\n",
    "            `test_dataloader` (AudioDataLoader)\n",
    "\n",
    "            `loss` (nn.Module)\n",
    "\n",
    "            `optimizer` (torch.optim.Optimizer)\n",
    "\n",
    "            `intervals` (int)\n",
    "\n",
    "            `save_path` (Path)\n",
    "\n",
    "            `early_stop_counter` (int)\n",
    "\n",
    "            `early_stopping_range` (int)\n",
    "\n",
    "            `epochs` (int)\n",
    "        \"\"\"\n",
    "        super().__init__(\n",
    "            semantic_encoder=semantic_encoder,\n",
    "            semantic_transformer=semantic_transformer,\n",
    "            acoustic_encoder_decoder=acoustic_encoder_decoder,\n",
    "            coarse_acoustic_transformer=coarse_acoustic_transformer,\n",
    "            fine_acoustic_transformer=fine_acoustic_transformer,\n",
    "            train_dataloader=train_dataloader,\n",
    "            val_dataloader=val_dataloader,\n",
    "            test_dataloader=test_dataloader,\n",
    "            loss=loss,\n",
    "            optimizer=optimizer,\n",
    "            intervals=intervals,\n",
    "            save_path=save_path,\n",
    "            early_stop_counter=early_stop_counter,\n",
    "            early_stopping_range=early_stopping_range,\n",
    "            epochs=epochs,\n",
    "        )\n",
    "        self.generate_audio_len = generate_audio_len\n",
    "\n",
    "    def loss_generator(self, batch):\n",
    "        semantic_encode = self.semantic_encoder(batch)\n",
    "        semantic_token = self.semantic_transformer.generate(semantic_encode, self.generate_audio_len * 50)\n",
    "\n",
    "        coarse_acoustic_tokens, fine_acoustic_tokens, _ = (\n",
    "            self.acoustic_encoder_decoder.encode(batch)\n",
    "        )\n",
    "        coarse_conditioning = torch.cat((semantic_token, coarse_acoustic_tokens), dim=1)\n",
    "        coarse_tokens = self.coarse_acoustic_transformer.generate(\n",
    "            coarse_conditioning, self.generate_audio_len * 75\n",
    "        )\n",
    "\n",
    "        output, target = self.fine_acoustic_transformer(\n",
    "            torch.cat((coarse_tokens, fine_acoustic_tokens), dim=1)\n",
    "        )\n",
    "        loss = self.loss(output, target)\n",
    "        return loss\n",
    "\n",
    "    def train(self):\n",
    "        return self._train(self.fine_acoustic_transformer)\n",
    "\n",
    "    def test(self):\n",
    "        return self._test(self.fine_acoustic_transformer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now define the full audiolm model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioLM:\n",
    "    def __init__(\n",
    "        self,\n",
    "        semantic_encoder: W2VHuBert,\n",
    "        semantic_transformer: SemanticTransformer,\n",
    "        acoustic_encoder_decoder: Encodec,\n",
    "        coarse_acoustic_transformer: CoarseAcousticTransformer,\n",
    "        fine_acoustic_transformer: FineAcousticTransformer,\n",
    "        # https://stackoverflow.com/a/53797072\n",
    "        *,\n",
    "        audio_len=1,\n",
    "        # We set Q' = 4 such that we predict the flattened tokens corresponding\n",
    "        # to the coarse 4 layers in the second stage.\n",
    "        n_coarse_quantizers=4,\n",
    "        # Not specified, but num quantizers must be a power of 2\n",
    "        # so this is the most reasonable combination.\n",
    "        n_fine_quantizers=4,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.semantic_encoder = semantic_encoder\n",
    "        for param in self.semantic_encoder.model.parameters():\n",
    "            param.requires_grad = False\n",
    "        self.acoustic_encoder_decoder = acoustic_encoder_decoder\n",
    "        for param in self.acoustic_encoder_decoder.model.parameters():\n",
    "            param.requires_grad = False\n",
    "        self.semantic_transformer = semantic_transformer\n",
    "        self.coarse_acoustic_transformer = coarse_acoustic_transformer\n",
    "        self.fine_acoustic_transformer = fine_acoustic_transformer\n",
    "        self.audio_len = audio_len\n",
    "        self.n_coarse_quantizers = n_coarse_quantizers\n",
    "        self.n_fine_quantizers = n_fine_quantizers\n",
    "\n",
    "    def generate(self, x: torch.Tensor, audio_len: int = 3):\n",
    "        \n",
    "        semantic_encode = self.semantic_encoder(x)\n",
    "        semantic_token = self.semantic_transformer.generate(\n",
    "            semantic_encode, audio_len * 50\n",
    "        )\n",
    "\n",
    "        coarse_acoustic_tokens, fine_acoustic_tokens, audio_scales = (\n",
    "            self.acoustic_encoder_decoder.encode(x)\n",
    "        )\n",
    "\n",
    "        coarse_conditioning = torch.cat((semantic_token, coarse_acoustic_tokens), dim=1)\n",
    "        coarse_tokens = self.coarse_acoustic_transformer.generate(\n",
    "            coarse_conditioning, audio_len * 75\n",
    "        )\n",
    "        \n",
    "        output = self.acoustic_encoder_decoder.decode(\n",
    "            coarse_tokens.unsqueeze(0).unsqueeze(0), [None]\n",
    "        )\n",
    "        return output[\"audio_values\"]\n",
    "    @staticmethod\n",
    "    def from_pretrained(\n",
    "        models_path: os.PathLike,\n",
    "        semantic_encoder : W2VHuBert,\n",
    "        acoustic_encoder_decoder : Encodec\n",
    "        ):\n",
    "        \n",
    "        semantic_transformer = SemanticTransformer()\n",
    "        state_dict = torch.load(\n",
    "            models_path / \"models\" / f\"{str(type(semantic_transformer).__name__)}.pth\"\n",
    "        )\n",
    "        semantic_transformer.load_state_dict(state_dict)\n",
    "        \n",
    "        coarse_acoustic_transformer = CoarseAcousticTransformer\n",
    "        state_dict = torch.load(\n",
    "            models_path\n",
    "            / \"models\"\n",
    "            / f\"{str(type(coarse_acoustic_transformer).__name__)}.pth\"\n",
    "        )\n",
    "        coarse_acoustic_transformer.load_state_dict(state_dict)\n",
    "        return AudioLM(\n",
    "            semantic_encoder=semantic_encoder,\n",
    "            semantic_transformer=semantic_transformer,\n",
    "            acoustic_encoder_decoder=acoustic_encoder_decoder,\n",
    "            coarse_acoustic_transformer=coarse_acoustic_transformer,\n",
    "            fine_acoustic_transformer=None,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now create a function for the instantation of the model assuming that the transformers are trained ahead of time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audiolm = AudioLM.from_pretrained(MODEL_PATH, semantic_encoder, acoustic_encoder_decoder)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
