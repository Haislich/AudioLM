{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AudioLM\n",
    "\n",
    "AudioLM is an high-quality audio generation framework, with long-term consistency. The main idea is to map an input audio sequence of discrete tokens in an intermediary discrete reppresentation space.\n",
    "One of the main issues that has been addressed in this work is the concilitation between generating audio with long-term consistency and and the recostruction of high quality audio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This are the necessary imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from audiolm.data_preparation import AudioDataLoader\n",
    "from audiolm.w2v_hubert import W2VHuBert\n",
    "from audiolm.absolute_transformer import (\n",
    "    SemanticTransformer,\n",
    "    CoarseAcousticTransformer,\n",
    "    FineAcousticTransformer,\n",
    ")\n",
    "from audiolm.encodec import Encodec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This are constants that will be important later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = Path(os.getcwd()) / Path(\"..\") / Path(\"data\") / Path(\"datasets\")\n",
    "MODEL_PATH = Path(os.getcwd()) / Path(\"..\") / Path(\"data\")\n",
    "INTERVALS = 10\n",
    "EARLY_STOP_COUNTER = 0\n",
    "EARLY_STOPPING_RANGE = 5\n",
    "EPOCHS = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both the semantic encoder are assumed pretrained and freezed ahead of time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\josed\\Documents\\AudioLM\\.venv\\lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:28: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
      "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n",
      "c:\\Users\\josed\\Documents\\AudioLM\\.venv\\lib\\site-packages\\sklearn\\base.py:376: InconsistentVersionWarning: Trying to unpickle estimator MiniBatchKMeans from version 0.24.0 when using version 1.4.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "c:\\Users\\josed\\Documents\\AudioLM\\.venv\\lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:28: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
      "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n",
      "c:\\Users\\josed\\Documents\\AudioLM\\.venv\\lib\\site-packages\\transformers\\models\\encodec\\modeling_encodec.py:120: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.register_buffer(\"padding_total\", torch.tensor(kernel_size - stride, dtype=torch.int64), persistent=False)\n"
     ]
    }
   ],
   "source": [
    "semantic_encoder = W2VHuBert()\n",
    "acoustic_encoder_decoder = Encodec()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The three dataloaders that will be needed, train val ,test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = AudioDataLoader(DATA_PATH / \"train\", batch_size=4,max_elems=10)\n",
    "val_dataloader = AudioDataLoader(DATA_PATH / \"val\", batch_size=4,max_elems=5)\n",
    "test_dataloader = AudioDataLoader(DATA_PATH / \"test\", batch_size=4,max_elems=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define what an abstract trainer should look like. This will give the skeleton for the specific trainers.\n",
    "This is needed because this framework uses a hierachical model approach, however at each level of the hierarchy only one transfomere is trained and much of the inherent logic is repeated.\n",
    "We can use this insight to create spoecialized classes for the training of a given transformer.\n",
    "The loss generator is the main component that will be changed the most during the specialization as it reflects how the input/output are created and is the only thing that changes between hierachies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from math import ceil\n",
    "from abc import ABC, abstractmethod\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from audiolm.encodec import Encodec\n",
    "from audiolm.constants import DEVICE\n",
    "from audiolm.data_preparation import AudioDataLoader\n",
    "from audiolm.w2v_hubert import W2VHuBert\n",
    "from audiolm.utils import save_checkpoint, save_model\n",
    "\n",
    "\n",
    "class Trainer(ABC):\n",
    "    \"\"\"\n",
    "    Trainer class for training a Transformer model.\n",
    "    \"\"\"\n",
    "\n",
    "    @abstractmethod\n",
    "    # pylint: disable =too-many-arguments\n",
    "    def __init__(\n",
    "        self,\n",
    "        semantic_encoder: Optional[W2VHuBert] = None,\n",
    "        semantic_transformer: Optional[SemanticTransformer] = None,\n",
    "        acoustic_encoder_decoder: Optional[Encodec] = None,\n",
    "        coarse_acoustic_transformer: Optional[CoarseAcousticTransformer] = None,\n",
    "        fine_acoustic_transformer: Optional[FineAcousticTransformer] = None,\n",
    "        train_dataloader: Optional[AudioDataLoader] = None,\n",
    "        val_dataloader: Optional[AudioDataLoader] = None,\n",
    "        test_dataloader: Optional[AudioDataLoader] = None,\n",
    "        loss: Optional[nn.Module] = None,\n",
    "        optimizer: Optional[torch.optim.Optimizer] = None,\n",
    "        intervals: Optional[int] = None,\n",
    "        save_path: Optional[os.PathLike] = None,\n",
    "        early_stop_counter: Optional[int] = None,\n",
    "        early_stopping_range: Optional[int] = None,\n",
    "        epochs: Optional[int] = None,\n",
    "    ):\n",
    "        self.semantic_encoder = semantic_encoder\n",
    "        self.semantic_transformer = semantic_transformer\n",
    "        self.acoustic_encoder_decoder = acoustic_encoder_decoder\n",
    "        self.coarse_acoustic_transformer = coarse_acoustic_transformer\n",
    "        self.fine_acoustic_transformer = fine_acoustic_transformer\n",
    "        self.train_dataloader = train_dataloader\n",
    "        self.val_dataloader = val_dataloader\n",
    "        self.test_dataloader = test_dataloader\n",
    "        self.optimizer = optimizer\n",
    "        self.intervals = intervals\n",
    "        self.epochs = epochs\n",
    "        self.save_path = save_path\n",
    "        self.best_val_loss = float(\"inf\")\n",
    "        self.early_stopping_range = early_stopping_range\n",
    "        self.early_stop_counter = early_stop_counter\n",
    "        self.loss = loss\n",
    "        if save_path is not None and not os.path.exists(self.save_path):\n",
    "            os.makedirs(self.save_path)\n",
    "\n",
    "    # region: Abstract methods, this methods must be redefined accordingly.\n",
    "    @abstractmethod\n",
    "    def loss_generator(self, batch):\n",
    "        \"\"\"Generate loss\"\"\"\n",
    "\n",
    "    @abstractmethod\n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Train the Transformer model.\n",
    "        \"\"\"\n",
    "\n",
    "    @abstractmethod\n",
    "    def test(self):\n",
    "        \"\"\"Test the model on the test dataset.\"\"\"\n",
    "\n",
    "    # endregion\n",
    "\n",
    "    # region: Private methods.\n",
    "    def _train_step(self, model: nn.Module) -> float:\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        \n",
    "        for batch in tqdm(self.train_dataloader, total = ceil(len(self.train_dataloader) / self.train_dataloader.batch_size) ):\n",
    "            batch = batch.to(DEVICE)\n",
    "            loss = self.loss_generator(batch)\n",
    "            train_loss += loss.item()\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "        train_loss /= len(self.train_dataloader)\n",
    "        return train_loss\n",
    "\n",
    "    def _validation_step(self, model: nn.Module) -> float:\n",
    "        model.eval()\n",
    "        validation_loss = 0\n",
    "        with torch.inference_mode():\n",
    "            for batch in self.val_dataloader:\n",
    "                batch = batch.to(DEVICE)\n",
    "                loss = self.loss_generator(batch)\n",
    "                validation_loss += loss.item()\n",
    "\n",
    "        validation_loss /= len(self.val_dataloader)\n",
    "\n",
    "        return validation_loss\n",
    "\n",
    "    def _train(self, model: nn.Module):\n",
    "        writer = SummaryWriter(Path(self.save_path) / \"runs\" / str(type(model).__name__))\n",
    "        for epoch in tqdm(range(self.epochs), total = self.epochs, desc=\"Training\"):\n",
    "            train_loss = self._train_step(model)\n",
    "            validation_loss = self._validation_step(model)\n",
    "            print(\"SAVING CHECKPOINT...\")\n",
    "            save_checkpoint(\n",
    "                model, epoch, self.optimizer, self.early_stop_counter, self.save_path\n",
    "            )\n",
    "            print(\"SAVING RUN FOR TENSORBOARD...\")\n",
    "            writer.add_scalars(\n",
    "                main_tag=f\"Loss_{str(type(model).__name__)}\",\n",
    "                tag_scalar_dict={\n",
    "                    \"train_loss\": train_loss,\n",
    "                    \"validation_loss\": validation_loss,\n",
    "                },\n",
    "                global_step=epoch,\n",
    "            )\n",
    "\n",
    "            if validation_loss < self.best_val_loss:\n",
    "                self.best_val_loss = validation_loss\n",
    "                self.early_stop_counter = 0\n",
    "            else:\n",
    "                self.early_stop_counter += 1\n",
    "\n",
    "            if self.early_stop_counter >= self.early_stopping_range:\n",
    "                print(f\"Early stopping training at epoch: {epoch+1}\")\n",
    "                break\n",
    "        writer.flush()\n",
    "        writer.close()\n",
    "        save_model(model, self.save_path)\n",
    "\n",
    "    def _test(self, model):\n",
    "        model.eval()\n",
    "        test_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(self.test_dataloader, desc=\"Testing\"):\n",
    "                batch = batch.to(DEVICE)\n",
    "                loss = self.loss_generator(batch)\n",
    "                test_loss += loss.item()\n",
    "\n",
    "        test_loss /= len(self.test_dataloader)\n",
    "        print(f\"Test Loss: {test_loss: .4f}\")\n",
    "\n",
    "        return test_loss\n",
    "\n",
    "    # endregion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first hierarchy of the model is used to train the semantic transformer used for autoregressive prediction of semantic tokens.\n",
    "The training is pretty straight forward in fact we only need to take the output of berts tokenization and feed into the transformer.\n",
    "`train` and `test` are overridden in order to specify which trasformer we're interessed to train and test rispectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SemanticTrainer(Trainer):\n",
    "    \"\"\"Trainer class derived from `Trainer`.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        semantic_encoder: W2VHuBert,\n",
    "        semantic_transformer: SemanticTransformer,\n",
    "        train_dataloader: AudioDataLoader,\n",
    "        val_dataloader: AudioDataLoader,\n",
    "        test_dataloader: AudioDataLoader,\n",
    "        loss: nn.Module,\n",
    "        optimizer: torch.optim.Optimizer,\n",
    "        intervals: int,\n",
    "        save_path: Path,\n",
    "        early_stop_counter: int,\n",
    "        early_stopping_range: int,\n",
    "        epochs: int,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Takes as input `semantic_encoder` and `semantic_transformer`.\n",
    "        They determine the `semantic_modelling`.\n",
    "\n",
    "        `semantic_encoder` must be trained ahead of time, this trainer only\n",
    "        trains `semantic_transformer`.\n",
    "\n",
    "        Args\n",
    "        ----\n",
    "            `semantic_encoder` (W2VHuBert)\n",
    "\n",
    "            `semantic_transformer` (TransformerDecoderOnly)\n",
    "\n",
    "            `train_dataloader` (AudioDataLoader)\n",
    "\n",
    "            `val_dataloader` (AudioDataLoader)\n",
    "\n",
    "            `test_dataloader` (AudioDataLoader)\n",
    "\n",
    "            `loss` (nn.Module)\n",
    "\n",
    "            `optimizer` (torch.optim.Optimizer)\n",
    "\n",
    "            `intervals` (int)\n",
    "\n",
    "            `save_path` (Path)\n",
    "\n",
    "            `early_stop_counter` (int)\n",
    "\n",
    "            `early_stopping_range` (int)\n",
    "\n",
    "            `epochs` (int)\n",
    "        \"\"\"\n",
    "        super().__init__(\n",
    "            semantic_encoder=semantic_encoder,\n",
    "            semantic_transformer=semantic_transformer,\n",
    "            train_dataloader=train_dataloader,\n",
    "            val_dataloader=val_dataloader,\n",
    "            test_dataloader=test_dataloader,\n",
    "            loss=loss,\n",
    "            optimizer=optimizer,\n",
    "            intervals=intervals,\n",
    "            save_path=save_path,\n",
    "            early_stop_counter=early_stop_counter,\n",
    "            early_stopping_range=early_stopping_range,\n",
    "            epochs=epochs,\n",
    "        )\n",
    "\n",
    "    def loss_generator(self, batch):\n",
    "        semantic_encode = self.semantic_encoder(batch)\n",
    "\n",
    "        output, target = self.semantic_transformer.fit(semantic_encode)\n",
    "\n",
    "        loss = self.loss(output, target)\n",
    "        return loss\n",
    "\n",
    "    def train(self):\n",
    "        return self._train(self.semantic_transformer)\n",
    "\n",
    "    def test(self):\n",
    "        return self._test(self.semantic_transformer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "training becomes a matter of instatiating the trainer and call train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5597ee553d3842ebb577ad0cf1315034",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d21780f0bcc046dfa2d42ab080686510",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/68 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a64472d9a0d4b9baadcafb60e564732",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/68 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01c7e5ed55f14caa99251c6b00f74775",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/68 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42f65ec797574c3fbedfee05e67f694d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/68 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbc77c5c06c94775a1f71a0516fdab1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/68 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "084afda76de54c04addd25058093a03c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/68 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d05472a4df704dfb90b2c2e8709fd404",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/68 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0439a58badfa4fa4bd353d4fed955977",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/68 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac847ac0fe264b96b6d79937db09bef5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/68 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b742f12692243169261552fc63b3dfb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/68 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "semantic_transformer = SemanticTransformer(num_heads=16,layers=12)\n",
    "semantic_loss = nn.CrossEntropyLoss()\n",
    "semantic_optimizer = torch.optim.Adam(\n",
    "        semantic_transformer.parameters(), lr=0.001\n",
    ")\n",
    "semantic_trainer = SemanticTrainer(\n",
    "            semantic_encoder=semantic_encoder,\n",
    "            semantic_transformer=semantic_transformer,\n",
    "            train_dataloader=train_dataloader,\n",
    "            val_dataloader=val_dataloader,\n",
    "            test_dataloader=test_dataloader,\n",
    "            loss=semantic_loss,\n",
    "            optimizer=semantic_optimizer,\n",
    "            intervals=INTERVALS,\n",
    "            save_path=MODEL_PATH,\n",
    "            early_stop_counter=EARLY_STOP_COUNTER,\n",
    "            early_stopping_range=EARLY_STOPPING_RANGE,\n",
    "            epochs=EPOCHS,\n",
    "        )\n",
    "semantic_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6007 (pid 22024), started 0:01:32 ago. (Use '!kill 22024' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-ba4f5f349e64cb5f\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-ba4f5f349e64cb5f\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 6007;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir=../data/runs/Semantic_Transformer/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second stage of this hierachy is the coarse acoustic modelling ... informazioni sul coarse.\n",
    "come funziona la generazione della loss i.e come avviene il condizionamento ecc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoarseAcousticTrainer(Trainer):\n",
    "    \"\"\"Trainer class derived from `Trainer`.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        semantic_encoder: W2VHuBert,\n",
    "        semantic_transformer: SemanticTransformer,\n",
    "        acoustic_encoder_decoder: Encodec,\n",
    "        coarse_acoustic_transformer: CoarseAcousticTransformer,\n",
    "        train_dataloader: AudioDataLoader,\n",
    "        val_dataloader: AudioDataLoader,\n",
    "        test_dataloader: AudioDataLoader,\n",
    "        loss: nn.Module,\n",
    "        optimizer: torch.optim.Optimizer,\n",
    "        intervals: int,\n",
    "        save_path: Path,\n",
    "        early_stop_counter: int,\n",
    "        early_stopping_range: int,\n",
    "        epochs: int,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Takes as input `semantic_encoder` and `semantic_transformer`.\n",
    "        They determine the `semantic_modelling`.\n",
    "\n",
    "        `semantic_encoder` must be trained ahead of time, this trainer only\n",
    "        trains `semantic_transformer`.\n",
    "\n",
    "        Args\n",
    "        ----\n",
    "            `semantic_encoder` (W2VHuBert)\n",
    "\n",
    "            `semantic_transformer` (TransformerDecoderOnly)\n",
    "\n",
    "            `train_dataloader` (AudioDataLoader)\n",
    "\n",
    "            `val_dataloader` (AudioDataLoader)\n",
    "\n",
    "            `test_dataloader` (AudioDataLoader)\n",
    "\n",
    "            `loss` (nn.Module)\n",
    "\n",
    "            `optimizer` (torch.optim.Optimizer)\n",
    "\n",
    "            `intervals` (int)\n",
    "\n",
    "            `save_path` (Path)\n",
    "\n",
    "            `early_stop_counter` (int)\n",
    "\n",
    "            `early_stopping_range` (int)\n",
    "\n",
    "            `epochs` (int)\n",
    "        \"\"\"\n",
    "        super().__init__(\n",
    "            semantic_encoder=semantic_encoder,\n",
    "            semantic_transformer=semantic_transformer,\n",
    "            acoustic_encoder_decoder=acoustic_encoder_decoder,\n",
    "            coarse_acoustic_transformer=coarse_acoustic_transformer,\n",
    "            train_dataloader=train_dataloader,\n",
    "            val_dataloader=val_dataloader,\n",
    "            test_dataloader=test_dataloader,\n",
    "            loss=loss,\n",
    "            optimizer=optimizer,\n",
    "            intervals=intervals,\n",
    "            save_path=save_path,\n",
    "            early_stop_counter=early_stop_counter,\n",
    "            early_stopping_range=early_stopping_range,\n",
    "            epochs=epochs,\n",
    "        )\n",
    "\n",
    "    def loss_generator(self, batch):\n",
    "\n",
    "        semantic_encode = self.semantic_encoder(batch)\n",
    "        print(semantic_encode.shape())\n",
    "        semantic_token = self.semantic_transformer.generate(semantic_encode, 3)\n",
    "       \n",
    "        coarse_acoustic_tokens, _, _ = self.acoustic_encoder_decoder.encode(batch)\n",
    "\n",
    "        conditioning = torch.cat((semantic_token, coarse_acoustic_tokens), dim=1)\n",
    "\n",
    "        output, target = self.coarse_acoustic_transformer.fit(conditioning)\n",
    "\n",
    "        loss = self.loss(output, target)\n",
    "        return loss\n",
    "\n",
    "    def train(self):\n",
    "        return self._train(self.coarse_acoustic_transformer)\n",
    "\n",
    "    def test(self):\n",
    "        return self._test(self.coarse_acoustic_transformer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\josed\\Documents\\AudioLM\\notebooks\\..\\data\\models\\SemanticTransformer.pth\n"
     ]
    }
   ],
   "source": [
    "print(MODEL_PATH / \"models\" / f\"{str(type(semantic_transformer).__name__)}.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "509ec2cb82dc4d09a6dbfd6c4bf43a3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1f3d54f35074d81b05e74c0c5e1b751",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/69 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "'torch.Size' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[66], line 27\u001b[0m\n\u001b[0;32m      8\u001b[0m coarse_optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(\n\u001b[0;32m      9\u001b[0m                 coarse_acoustic_transformer\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m\n\u001b[0;32m     10\u001b[0m             )\n\u001b[0;32m     11\u001b[0m coarse_acoustic_trainer \u001b[38;5;241m=\u001b[39m CoarseAcousticTrainer(\n\u001b[0;32m     12\u001b[0m                 semantic_encoder\u001b[38;5;241m=\u001b[39msemantic_encoder,\n\u001b[0;32m     13\u001b[0m                 semantic_transformer\u001b[38;5;241m=\u001b[39msemantic_transformer,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     25\u001b[0m                 epochs\u001b[38;5;241m=\u001b[39mEPOCHS,\n\u001b[0;32m     26\u001b[0m             )\n\u001b[1;32m---> 27\u001b[0m \u001b[43mcoarse_acoustic_trainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[63], line 87\u001b[0m, in \u001b[0;36mCoarseAcousticTrainer.train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m---> 87\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcoarse_acoustic_transformer\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[45], line 109\u001b[0m, in \u001b[0;36mTrainer._train\u001b[1;34m(self, model)\u001b[0m\n\u001b[0;32m    107\u001b[0m writer \u001b[38;5;241m=\u001b[39m SummaryWriter(Path(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_path) \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mruns\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mtype\u001b[39m(model)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m))\n\u001b[0;32m    108\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepochs), total \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepochs, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 109\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    110\u001b[0m     validation_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validation_step(model)\n\u001b[0;32m    111\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSAVING CHECKPOINT...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[45], line 85\u001b[0m, in \u001b[0;36mTrainer._train_step\u001b[1;34m(self, model)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_dataloader, total \u001b[38;5;241m=\u001b[39m ceil(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_dataloader) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_dataloader\u001b[38;5;241m.\u001b[39mbatch_size) ):\n\u001b[0;32m     84\u001b[0m     batch \u001b[38;5;241m=\u001b[39m batch\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[1;32m---> 85\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss_generator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     86\u001b[0m     train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m     87\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "Cell \u001b[1;32mIn[63], line 74\u001b[0m, in \u001b[0;36mCoarseAcousticTrainer.loss_generator\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mloss_generator\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch):\n\u001b[0;32m     73\u001b[0m     semantic_encode \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msemantic_encoder(batch)\n\u001b[1;32m---> 74\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[43msemantic_encode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     75\u001b[0m     semantic_token \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msemantic_transformer\u001b[38;5;241m.\u001b[39mgenerate(semantic_encode, \u001b[38;5;241m3\u001b[39m)\n\u001b[0;32m     77\u001b[0m     coarse_acoustic_tokens, _, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39macoustic_encoder_decoder\u001b[38;5;241m.\u001b[39mencode(batch)\n",
      "\u001b[1;31mTypeError\u001b[0m: 'torch.Size' object is not callable"
     ]
    }
   ],
   "source": [
    "semantic_transformer = SemanticTransformer()\n",
    "state_dict = torch.load(\n",
    "            MODEL_PATH / \"models\" / f\"{str(type(semantic_transformer).__name__)}.pth\"\n",
    "        )\n",
    "semantic_transformer.load_state_dict(state_dict)\n",
    "coarse_acoustic_transformer = CoarseAcousticTransformer(num_heads=16,layers=12)\n",
    "coarse_loss = nn.CrossEntropyLoss()\n",
    "coarse_optimizer = torch.optim.Adam(\n",
    "                coarse_acoustic_transformer.parameters(), lr=0.001\n",
    "            )\n",
    "coarse_acoustic_trainer = CoarseAcousticTrainer(\n",
    "                semantic_encoder=semantic_encoder,\n",
    "                semantic_transformer=semantic_transformer,\n",
    "                acoustic_encoder_decoder=acoustic_encoder_decoder,\n",
    "                coarse_acoustic_transformer=coarse_acoustic_transformer,\n",
    "                train_dataloader=train_dataloader,\n",
    "                val_dataloader=val_dataloader,\n",
    "                test_dataloader=test_dataloader,\n",
    "                loss=coarse_loss,\n",
    "                optimizer=coarse_optimizer,\n",
    "                intervals=INTERVALS,\n",
    "                save_path=MODEL_PATH,\n",
    "                early_stop_counter=EARLY_STOP_COUNTER,\n",
    "                early_stopping_range=EARLY_STOPPING_RANGE,\n",
    "                epochs=EPOCHS,\n",
    "            )\n",
    "coarse_acoustic_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "275\n"
     ]
    }
   ],
   "source": [
    "print(len(train_dataloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the sake of completness we implement the Fine acoustic trainer, even though it won't be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FineAcousticTrainer(Trainer):\n",
    "    \"\"\"Trainer class derived from `Trainer`.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        semantic_encoder: W2VHuBert,\n",
    "        semantic_transformer: SemanticTransformer,\n",
    "        acoustic_encoder_decoder: Encodec,\n",
    "        coarse_acoustic_transformer: CoarseAcousticTransformer,\n",
    "        fine_acoustic_transformer: FineAcousticTransformer,\n",
    "        train_dataloader: AudioDataLoader,\n",
    "        val_dataloader: AudioDataLoader,\n",
    "        test_dataloader: AudioDataLoader,\n",
    "        loss: nn.Module,\n",
    "        optimizer: torch.optim.Optimizer,\n",
    "        intervals: int,\n",
    "        save_path: Path,\n",
    "        early_stop_counter: int,\n",
    "        early_stopping_range: int,\n",
    "        epochs: int,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Takes as input `semantic_encoder` and `semantic_transformer`.\n",
    "        They determine the `semantic_modelling`.\n",
    "\n",
    "        `semantic_encoder` must be trained ahead of time, this trainer only\n",
    "        trains `semantic_transformer`.\n",
    "\n",
    "        Args\n",
    "        ----\n",
    "            `semantic_encoder` (W2VHuBert)\n",
    "\n",
    "            `semantic_transformer` (TransformerDecoderOnly)\n",
    "\n",
    "            `train_dataloader` (AudioDataLoader)\n",
    "\n",
    "            `val_dataloader` (AudioDataLoader)\n",
    "\n",
    "            `test_dataloader` (AudioDataLoader)\n",
    "\n",
    "            `loss` (nn.Module)\n",
    "\n",
    "            `optimizer` (torch.optim.Optimizer)\n",
    "\n",
    "            `intervals` (int)\n",
    "\n",
    "            `save_path` (Path)\n",
    "\n",
    "            `early_stop_counter` (int)\n",
    "\n",
    "            `early_stopping_range` (int)\n",
    "\n",
    "            `epochs` (int)\n",
    "        \"\"\"\n",
    "        super().__init__(\n",
    "            semantic_encoder=semantic_encoder,\n",
    "            semantic_transformer=semantic_transformer,\n",
    "            acoustic_encoder_decoder=acoustic_encoder_decoder,\n",
    "            coarse_acoustic_transformer=coarse_acoustic_transformer,\n",
    "            fine_acoustic_transformer=fine_acoustic_transformer,\n",
    "            train_dataloader=train_dataloader,\n",
    "            val_dataloader=val_dataloader,\n",
    "            test_dataloader=test_dataloader,\n",
    "            loss=loss,\n",
    "            optimizer=optimizer,\n",
    "            intervals=intervals,\n",
    "            save_path=save_path,\n",
    "            early_stop_counter=early_stop_counter,\n",
    "            early_stopping_range=early_stopping_range,\n",
    "            epochs=epochs,\n",
    "        )\n",
    "\n",
    "    def loss_generator(self, batch):\n",
    "        semantic_encode = self.semantic_encoder(batch)\n",
    "        semantic_token = self.semantic_transformer.generate(semantic_encode, 3)\n",
    "\n",
    "        coarse_acoustic_tokens, fine_acoustic_tokens, _ = (\n",
    "            self.acoustic_encoder_decoder.encode(batch)\n",
    "        )\n",
    "        coarse_conditioning = torch.cat((semantic_token, coarse_acoustic_tokens), dim=1)\n",
    "        coarse_tokens = self.coarse_acoustic_transformer.generate(\n",
    "            coarse_conditioning, 3\n",
    "        )\n",
    "\n",
    "        output, target = self.fine_acoustic_transformer(\n",
    "            torch.cat((coarse_tokens, fine_acoustic_tokens), dim=1)\n",
    "        )\n",
    "        loss = self.loss(output, target)\n",
    "        return loss\n",
    "\n",
    "    def train(self):\n",
    "        return self._train(self.fine_acoustic_transformer)\n",
    "\n",
    "    def test(self):\n",
    "        return self._test(self.fine_acoustic_transformer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now define the full audiolm model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioLM:\n",
    "    def __init__(\n",
    "        self,\n",
    "        semantic_encoder: W2VHuBert,\n",
    "        semantic_transformer: SemanticTransformer,\n",
    "        acoustic_encoder_decoder: Encodec,\n",
    "        coarse_acoustic_transformer: CoarseAcousticTransformer,\n",
    "        fine_acoustic_transformer: FineAcousticTransformer,\n",
    "        # https://stackoverflow.com/a/53797072\n",
    "        *,\n",
    "        audio_len=1,\n",
    "        # We set Q' = 4 such that we predict the flattened tokens corresponding\n",
    "        # to the coarse 4 layers in the second stage.\n",
    "        n_coarse_quantizers=4,\n",
    "        # Not specified, but num quantizers must be a power of 2\n",
    "        # so this is the most reasonable combination.\n",
    "        n_fine_quantizers=4,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.semantic_encoder = semantic_encoder\n",
    "        for param in self.semantic_encoder.model.parameters():\n",
    "            param.requires_grad = False\n",
    "        self.acoustic_encoder_decoder = acoustic_encoder_decoder\n",
    "        for param in self.acoustic_encoder_decoder.model.parameters():\n",
    "            param.requires_grad = False\n",
    "        self.semantic_transformer = semantic_transformer\n",
    "        self.coarse_acoustic_transformer = coarse_acoustic_transformer\n",
    "        self.fine_acoustic_transformer = fine_acoustic_transformer\n",
    "        self.audio_len = audio_len\n",
    "        self.n_coarse_quantizers = n_coarse_quantizers\n",
    "        self.n_fine_quantizers = n_fine_quantizers\n",
    "\n",
    "    def generate(self, x: torch.Tensor, audio_len: int = 3):\n",
    "        \n",
    "        semantic_encode = self.semantic_encoder(x)\n",
    "        semantic_token = self.semantic_transformer.generate(\n",
    "            semantic_encode, audio_len * 50\n",
    "        )\n",
    "\n",
    "        coarse_acoustic_tokens, fine_acoustic_tokens, audio_scales = (\n",
    "            self.acoustic_encoder_decoder.encode(x)\n",
    "        )\n",
    "\n",
    "        coarse_conditioning = torch.cat((semantic_token, coarse_acoustic_tokens), dim=1)\n",
    "        coarse_tokens = self.coarse_acoustic_transformer.generate(\n",
    "            coarse_conditioning, audio_len * 75\n",
    "        )\n",
    "        \n",
    "        output = self.acoustic_encoder_decoder.decode(\n",
    "            coarse_tokens.unsqueeze(0).unsqueeze(0), [None]\n",
    "        )\n",
    "        return output[\"audio_values\"]\n",
    "    @staticmethod\n",
    "    def from_pretrained(\n",
    "        models_path: os.PathLike,\n",
    "        semantic_encoder : W2VHuBert,\n",
    "        acoustic_encoder_decoder : Encodec\n",
    "        ):\n",
    "        \n",
    "        semantic_transformer = SemanticTransformer()\n",
    "        state_dict = torch.load(\n",
    "            models_path / \"models\" / f\"{str(type(semantic_transformer).__name__)}.pth\"\n",
    "        )\n",
    "        semantic_transformer.load_state_dict(state_dict)\n",
    "        \n",
    "        coarse_acoustic_transformer = CoarseAcousticTransformer()\n",
    "        state_dict = torch.load(\n",
    "            models_path\n",
    "            / \"models\"\n",
    "            / f\"{str(type(coarse_acoustic_transformer).__name__)}.pth\"\n",
    "        )\n",
    "        coarse_acoustic_transformer.load_state_dict(state_dict)\n",
    "        return AudioLM(\n",
    "            semantic_encoder=semantic_encoder,\n",
    "            semantic_transformer=semantic_transformer,\n",
    "            acoustic_encoder_decoder=acoustic_encoder_decoder,\n",
    "            coarse_acoustic_transformer=coarse_acoustic_transformer,\n",
    "            fine_acoustic_transformer=None,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now create a function for the instantation of the model assuming that the transformers are trained ahead of time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audiolm = AudioLM.from_pretrained(MODEL_PATH, semantic_encoder,acoustic_encoder_decoder)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
