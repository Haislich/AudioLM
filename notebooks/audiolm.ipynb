{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## AudioLM: A Language Modeling Approach to Audio Generation\n",
    "\n",
    "**AudioLM** represents a self-supervised learning methodology aimed at generating high-quality audio with sustained long-term consistency. This technique maps input audio to a sequence of discrete tokens, treating the audio generation process akin to a language modeling task within this representational framework. The proposal introduces a hybrid tokenization scheme, employing the discretized activations of a pre-trained masked language model on audio to capture long-term structures, and discrete codes from a neural audio codec to ensure high-fidelity synthesis.\n",
    "\n",
    "#### Tokenization Approach:\n",
    "The core innovation of this method lies in the **Hybrid Tokenization Scheme**, crucial for subsequent conditioning of transformers.\n",
    "\n",
    "**Hybrid Tokenization Scheme**: AudioLM combines semantic and acoustic tokens hierarchically to strike a balance between long-term consistency and high-quality audio synthesis. The semantic tokens are extracted from a pre-trained w2v-BERT model, while the acoustic tokens are derived from a finely-tuned SoundStream on a speech dataset. This approach emphasizes their complementary strengths in phonetic discriminability and reconstruction quality.\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"images/semantic_acoustic_tokens.png\" alt=\"Hybrid Tokenization Scheme\" width=\"400\">\n",
    "</div>\n",
    "\n",
    "#### Hybrid Tokenization:\n",
    "As depicted in the image, the hybrid tokenization scheme is divided into two parallel components:\n",
    "1. **Semantic Tokens: W2V-Bert-based tokenizer**\n",
    "   This component is responsible for extracting features from audio waveforms as a 1024-dimensional embedding. A K-means quantizer with 1024 clusters discretizes these embeddings. Each feature vector in the space is associated with a reference cluster based on its proximity, thus identifying the i-th **semantic token**.\n",
    "\n",
    "2. **Acoustic Tokens: Soundstream tokenizer**\n",
    "    .....\n",
    "\n",
    "#### Autoregressive Prediction:\n",
    "The prediction phase is subsequently divided into three distinct stages:\n",
    "1. **Semantic Stage:** This initial stage utilizes semantic tokenization to train a decoder-only transformer. By receiving a sequence of semantic tokens, it performs an autoregressive prediction task during inference. Specifically, the first stage models $$p(z_t|z_{t-1})$$, focusing on the autoregressive prediction of semantic tokens to capture long-term temporal structure.\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"images/semantic_stage.png\" alt=\"Semantic Stage\" width=\"400\">\n",
    "</div>\n",
    "\n",
    "2. **Coarse Acoustic Stage:** Similar to the first, this stage performs an autoregressive prediction task; however, unlike the first, the second transformer receives a sequence of acoustic tokens input from the first four quantizers of Soundstream, conditioned by the previously generated semantic tokens. Specifically, the acoustic tokens possess a hierarchical structure where coarse quantizers capture fundamental acoustic properties such as speaker identity and recording conditions. The second stage models $$p(y_t | z, y_{<t}, y_t)$$ for $q \\leq Q'$, representing the token sequence as $(z_1, z_2, \\ldots, z_{T_s}, y_1^{1}, y_1^{2}, \\ldots, y_{Q'}^{1}, y_2^{1}, \\ldots, y_{T_A}^{Q'})$, with $y_1$ being the first token predicted during training.\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"images/acoustic_stage.png\" alt=\"Coarse Acoustic Stage\" width=\"400\">\n",
    "</div>\n",
    "\n",
    "3. **Fine Acoustic Stage and Decoding:** Finally, this stage significantly improves audio quality by eliminating the lossy compression artifacts produced during the second stage. Here, the prompt and the sampled acoustic tokens are fed into the **SoundStream** decoder to reconstruct a waveform $\\hat x$.\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"images/decoding.png\" alt=\"Fine Acoustic Stage and Decoding\" width=\"400\">\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-03 15:27:14 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from audiolm.data_preparation import AudioDataLoader\n",
    "from audiolm.w2v_hubert import W2VHuBert\n",
    "from audiolm.absolute_transformer import (\n",
    "    SemanticTransformer,\n",
    "    CoarseAcousticTransformer,\n",
    "    FineAcousticTransformer,\n",
    ")\n",
    "from audiolm.encodec import Encodec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DATA_PATH = Path(os.getcwd()) / Path(\"..\") / Path(\"data\") / Path(\"datasets\")\n",
    "#MODEL_PATH = Path(os.getcwd()) / Path(\"..\") / Path(\"data\")\n",
    "DATA_PATH = Path(\"/Volumes/SSD-EXT/NN_project/dataset\")\n",
    "MODEL_PATH = Path(\"/Volumes/SSD-EXT/NN_project/models\")\n",
    "INTERVALS = 10\n",
    "EARLY_STOP_COUNTER = 0\n",
    "EARLY_STOPPING_RANGE = 5\n",
    "EPOCHS = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantic and Acoustic encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/valerio/Desktop/AudioLM/.venv/lib/python3.9/site-packages/torch/nn/utils/weight_norm.py:28: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
      "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n",
      "/Users/valerio/Desktop/AudioLM/.venv/lib/python3.9/site-packages/threadpoolctl.py:1214: RuntimeWarning: \n",
      "Found Intel OpenMP ('libiomp') and LLVM OpenMP ('libomp') loaded at\n",
      "the same time. Both libraries are known to be incompatible and this\n",
      "can cause random crashes or deadlocks on Linux when loaded in the\n",
      "same Python program.\n",
      "Using threadpoolctl may cause crashes or deadlocks. For more\n",
      "information and possible workarounds, please see\n",
      "    https://github.com/joblib/threadpoolctl/blob/master/multiple_openmp.md\n",
      "\n",
      "  warnings.warn(msg, RuntimeWarning)\n",
      "/Users/valerio/Desktop/AudioLM/.venv/lib/python3.9/site-packages/sklearn/base.py:376: InconsistentVersionWarning: Trying to unpickle estimator MiniBatchKMeans from version 0.24.0 when using version 1.5.0. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/Users/valerio/Desktop/AudioLM/.venv/lib/python3.9/site-packages/torch/nn/utils/weight_norm.py:28: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
      "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n",
      "/Users/valerio/Desktop/AudioLM/.venv/lib/python3.9/site-packages/transformers/models/encodec/modeling_encodec.py:120: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.register_buffer(\"padding_total\", torch.tensor(kernel_size - stride, dtype=torch.int64), persistent=False)\n"
     ]
    }
   ],
   "source": [
    "semantic_encoder = W2VHuBert()\n",
    "acoustic_encoder_decoder = Encodec()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = AudioDataLoader(DATA_PATH / \"train\", batch_size=6,max_elems=35)\n",
    "val_dataloader = AudioDataLoader(DATA_PATH / \"val\", batch_size=6,max_elems=15)\n",
    "test_dataloader = AudioDataLoader(DATA_PATH / \"test\", batch_size=6,max_elems=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define what an abstract trainer should look like. This will give the skeleton for the specific trainers.\n",
    "This is needed because this framework uses a hierachical model approach, however at each level of the hierarchy only one transfomere is trained and much of the inherent logic is repeated.\n",
    "We can use this insight to create spoecialized classes for the training of a given transformer.\n",
    "The loss generator is the main component that will be changed the most during the specialization as it reflects how the input/output are created and is the only thing that changes between hierachies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainer class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from math import ceil\n",
    "from abc import ABC, abstractmethod\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from audiolm.encodec import Encodec\n",
    "from audiolm.constants import DEVICE\n",
    "from audiolm.data_preparation import AudioDataLoader\n",
    "from audiolm.w2v_hubert import W2VHuBert\n",
    "from audiolm.utils import save_checkpoint, save_model\n",
    "\n",
    "\n",
    "class Trainer(ABC):\n",
    "    \"\"\"\n",
    "    Trainer class for training a Transformer model.\n",
    "    \"\"\"\n",
    "\n",
    "    @abstractmethod\n",
    "    # pylint: disable =too-many-arguments\n",
    "    def __init__(\n",
    "        self,\n",
    "        semantic_encoder: Optional[W2VHuBert] = None,\n",
    "        semantic_transformer: Optional[SemanticTransformer] = None,\n",
    "        acoustic_encoder_decoder: Optional[Encodec] = None,\n",
    "        coarse_acoustic_transformer: Optional[CoarseAcousticTransformer] = None,\n",
    "        fine_acoustic_transformer: Optional[FineAcousticTransformer] = None,\n",
    "        train_dataloader: Optional[AudioDataLoader] = None,\n",
    "        val_dataloader: Optional[AudioDataLoader] = None,\n",
    "        test_dataloader: Optional[AudioDataLoader] = None,\n",
    "        loss: Optional[nn.Module] = None,\n",
    "        optimizer: Optional[torch.optim.Optimizer] = None,\n",
    "        intervals: Optional[int] = None,\n",
    "        save_path: Optional[os.PathLike] = None,\n",
    "        early_stop_counter: Optional[int] = None,\n",
    "        early_stopping_range: Optional[int] = None,\n",
    "        epochs: Optional[int] = None,\n",
    "    ):\n",
    "        self.semantic_encoder = semantic_encoder\n",
    "        self.semantic_transformer = semantic_transformer\n",
    "        self.acoustic_encoder_decoder = acoustic_encoder_decoder\n",
    "        self.coarse_acoustic_transformer = coarse_acoustic_transformer\n",
    "        self.fine_acoustic_transformer = fine_acoustic_transformer\n",
    "        self.train_dataloader = train_dataloader\n",
    "        self.val_dataloader = val_dataloader\n",
    "        self.test_dataloader = test_dataloader\n",
    "        self.optimizer = optimizer\n",
    "        self.intervals = intervals\n",
    "        self.epochs = epochs\n",
    "        self.save_path = save_path\n",
    "        self.best_val_loss = float(\"inf\")\n",
    "        self.early_stopping_range = early_stopping_range\n",
    "        self.early_stop_counter = early_stop_counter\n",
    "        self.loss = loss\n",
    "        if save_path is not None and not os.path.exists(self.save_path):\n",
    "            os.makedirs(self.save_path)\n",
    "\n",
    "    # region: Abstract methods, this methods must be redefined accordingly.\n",
    "    @abstractmethod\n",
    "    def loss_generator(self, batch):\n",
    "        \"\"\"Generate loss\"\"\"\n",
    "\n",
    "    @abstractmethod\n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Train the Transformer model.\n",
    "        \"\"\"\n",
    "\n",
    "    @abstractmethod\n",
    "    def test(self):\n",
    "        \"\"\"Test the model on the test dataset.\"\"\"\n",
    "\n",
    "    # endregion\n",
    "\n",
    "    # region: Private methods.\n",
    "    def _train_step(self, model: nn.Module) -> float:\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        \n",
    "        for batch in tqdm(self.train_dataloader, total = ceil(len(self.train_dataloader) / self.train_dataloader.batch_size) ):\n",
    "            batch = batch.to(DEVICE)\n",
    "            loss = self.loss_generator(batch)\n",
    "            train_loss += loss.item()\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "        train_loss /= len(self.train_dataloader)\n",
    "        return train_loss\n",
    "\n",
    "    def _validation_step(self, model: nn.Module) -> float:\n",
    "        model.eval()\n",
    "        validation_loss = 0\n",
    "        with torch.inference_mode():\n",
    "            for batch in self.val_dataloader:\n",
    "                batch = batch.to(DEVICE)\n",
    "                loss = self.loss_generator(batch)\n",
    "                validation_loss += loss.item()\n",
    "\n",
    "        validation_loss /= len(self.val_dataloader)\n",
    "\n",
    "        return validation_loss\n",
    "\n",
    "    def _train(self, model: nn.Module):\n",
    "        writer = SummaryWriter(Path(self.save_path) / \"runs\" / str(type(model).__name__))\n",
    "        for epoch in tqdm(range(self.epochs), total = self.epochs, desc=\"Training\"):\n",
    "            train_loss = self._train_step(model)\n",
    "            validation_loss = self._validation_step(model)\n",
    "            print(\"SAVING CHECKPOINT...\")\n",
    "            save_checkpoint(\n",
    "                model, epoch, self.optimizer, self.early_stop_counter, self.save_path\n",
    "            )\n",
    "            print(\"SAVING RUN FOR TENSORBOARD...\")\n",
    "            writer.add_scalars(\n",
    "                main_tag=f\"Loss_{str(type(model).__name__)}\",\n",
    "                tag_scalar_dict={\n",
    "                    \"train_loss\": train_loss,\n",
    "                    \"validation_loss\": validation_loss,\n",
    "                },\n",
    "                global_step=epoch,\n",
    "            )\n",
    "\n",
    "            if validation_loss < self.best_val_loss:\n",
    "                self.best_val_loss = validation_loss\n",
    "                self.early_stop_counter = 0\n",
    "            else:\n",
    "                self.early_stop_counter += 1\n",
    "\n",
    "            if self.early_stop_counter >= self.early_stopping_range:\n",
    "                print(f\"Early stopping training at epoch: {epoch+1}\")\n",
    "                break\n",
    "        writer.flush()\n",
    "        writer.close()\n",
    "        save_model(model, self.save_path)\n",
    "\n",
    "    def _test(self, model):\n",
    "        model.eval()\n",
    "        test_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(self.test_dataloader, desc=\"Testing\"):\n",
    "                batch = batch.to(DEVICE)\n",
    "                loss = self.loss_generator(batch)\n",
    "                test_loss += loss.item()\n",
    "\n",
    "        test_loss /= len(self.test_dataloader)\n",
    "        print(f\"Test Loss: {test_loss: .4f}\")\n",
    "\n",
    "        return test_loss\n",
    "\n",
    "    # endregion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantic Trainer class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SemanticTrainer(Trainer):\n",
    "    \"\"\"Trainer class derived from `Trainer`.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        semantic_encoder: W2VHuBert,\n",
    "        semantic_transformer: SemanticTransformer,\n",
    "        train_dataloader: AudioDataLoader,\n",
    "        val_dataloader: AudioDataLoader,\n",
    "        test_dataloader: AudioDataLoader,\n",
    "        loss: nn.Module,\n",
    "        optimizer: torch.optim.Optimizer,\n",
    "        intervals: int,\n",
    "        save_path: Path,\n",
    "        early_stop_counter: int,\n",
    "        early_stopping_range: int,\n",
    "        epochs: int,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Takes as input `semantic_encoder` and `semantic_transformer`.\n",
    "        They determine the `semantic_modelling`.\n",
    "\n",
    "        `semantic_encoder` must be trained ahead of time, this trainer only\n",
    "        trains `semantic_transformer`.\n",
    "\n",
    "        Args\n",
    "        ----\n",
    "            `semantic_encoder` (W2VHuBert)\n",
    "\n",
    "            `semantic_transformer` (TransformerDecoderOnly)\n",
    "\n",
    "            `train_dataloader` (AudioDataLoader)\n",
    "\n",
    "            `val_dataloader` (AudioDataLoader)\n",
    "\n",
    "            `test_dataloader` (AudioDataLoader)\n",
    "\n",
    "            `loss` (nn.Module)\n",
    "\n",
    "            `optimizer` (torch.optim.Optimizer)\n",
    "\n",
    "            `intervals` (int)\n",
    "\n",
    "            `save_path` (Path)\n",
    "\n",
    "            `early_stop_counter` (int)\n",
    "\n",
    "            `early_stopping_range` (int)\n",
    "\n",
    "            `epochs` (int)\n",
    "        \"\"\"\n",
    "        super().__init__(\n",
    "            semantic_encoder=semantic_encoder,\n",
    "            semantic_transformer=semantic_transformer,\n",
    "            train_dataloader=train_dataloader,\n",
    "            val_dataloader=val_dataloader,\n",
    "            test_dataloader=test_dataloader,\n",
    "            loss=loss,\n",
    "            optimizer=optimizer,\n",
    "            intervals=intervals,\n",
    "            save_path=save_path,\n",
    "            early_stop_counter=early_stop_counter,\n",
    "            early_stopping_range=early_stopping_range,\n",
    "            epochs=epochs,\n",
    "        )\n",
    "\n",
    "    def loss_generator(self, batch):\n",
    "        semantic_encode = self.semantic_encoder(batch)\n",
    "\n",
    "        output, target = self.semantic_transformer.fit(semantic_encode)\n",
    "\n",
    "        loss = self.loss(output, target)\n",
    "        return loss\n",
    "\n",
    "    def train(self):\n",
    "        return self._train(self.semantic_transformer)\n",
    "\n",
    "    def test(self):\n",
    "        return self._test(self.semantic_transformer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cd01ead296440fa817f4e05b8f9754a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c1de183e1ee4800ac67cfb0a839b519",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/137 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAVING CHECKPOINT...\n",
      "SAVING RUN FOR TENSORBOARD...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bc7b227ef71489e8dc820fc8c438b9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/137 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAVING CHECKPOINT...\n",
      "SAVING RUN FOR TENSORBOARD...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a811f72ae2674bf5827f68d8b1660f40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/137 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAVING CHECKPOINT...\n",
      "SAVING RUN FOR TENSORBOARD...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4a6f43129354115a85dd03e5aa3ba47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/137 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAVING CHECKPOINT...\n",
      "SAVING RUN FOR TENSORBOARD...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e72d862e3e840e29260377a37ef9c88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/137 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAVING CHECKPOINT...\n",
      "SAVING RUN FOR TENSORBOARD...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09539e82e4a14b798eb2d5ef7b67607c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/137 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAVING CHECKPOINT...\n",
      "SAVING RUN FOR TENSORBOARD...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f65e58493b040b1b0d80513b6f82b44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/137 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAVING CHECKPOINT...\n",
      "SAVING RUN FOR TENSORBOARD...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f2ed948ccf44406b648392fa19385ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/137 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAVING CHECKPOINT...\n",
      "SAVING RUN FOR TENSORBOARD...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22b739a03e7f49c78ecbd62704aa3772",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/137 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "semantic_transformer_model = SemanticTransformer(num_heads=4,layers=2)\n",
    "semantic_loss = nn.CrossEntropyLoss()\n",
    "semantic_optimizer = torch.optim.Adam(\n",
    "        semantic_transformer_model.parameters(), lr=0.001\n",
    ")\n",
    "semantic_trainer = SemanticTrainer(\n",
    "            semantic_encoder=semantic_encoder,\n",
    "            semantic_transformer=semantic_transformer_model,\n",
    "            train_dataloader=train_dataloader,\n",
    "            val_dataloader=val_dataloader,\n",
    "            test_dataloader=test_dataloader,\n",
    "            loss=semantic_loss,\n",
    "            optimizer=semantic_optimizer,\n",
    "            intervals=INTERVALS,\n",
    "            save_path=MODEL_PATH,\n",
    "            early_stop_counter=EARLY_STOP_COUNTER,\n",
    "            early_stopping_range=EARLY_STOPPING_RANGE,\n",
    "            epochs=EPOCHS,\n",
    "        )\n",
    "semantic_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Launching TensorBoard..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir=../data/runs/Semantic_Transformer/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second stage of this hierachy is the coarse acoustic modelling ... informazioni sul coarse.\n",
    "come funziona la generazione della loss i.e come avviene il condizionamento ecc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coarse Acoustic Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoarseAcousticTrainer(Trainer):\n",
    "    \"\"\"Trainer class derived from `Trainer`.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        semantic_encoder: W2VHuBert,\n",
    "        semantic_transformer: SemanticTransformer,\n",
    "        acoustic_encoder_decoder: Encodec,\n",
    "        coarse_acoustic_transformer: CoarseAcousticTransformer,\n",
    "        train_dataloader: AudioDataLoader,\n",
    "        val_dataloader: AudioDataLoader,\n",
    "        test_dataloader: AudioDataLoader,\n",
    "        loss: nn.Module,\n",
    "        optimizer: torch.optim.Optimizer,\n",
    "        intervals: int,\n",
    "        save_path: Path,\n",
    "        early_stop_counter: int,\n",
    "        early_stopping_range: int,\n",
    "        generate_audio_len: int,\n",
    "        epochs: int,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Takes as input `semantic_encoder` and `semantic_transformer`.\n",
    "        They determine the `semantic_modelling`.\n",
    "\n",
    "        `semantic_encoder` must be trained ahead of time, this trainer only\n",
    "        trains `semantic_transformer`.\n",
    "\n",
    "        Args\n",
    "        ----\n",
    "            `semantic_encoder` (W2VHuBert)\n",
    "\n",
    "            `semantic_transformer` (TransformerDecoderOnly)\n",
    "\n",
    "            `train_dataloader` (AudioDataLoader)\n",
    "\n",
    "            `val_dataloader` (AudioDataLoader)\n",
    "\n",
    "            `test_dataloader` (AudioDataLoader)\n",
    "\n",
    "            `loss` (nn.Module)\n",
    "\n",
    "            `optimizer` (torch.optim.Optimizer)\n",
    "\n",
    "            `intervals` (int)\n",
    "\n",
    "            `save_path` (Path)\n",
    "\n",
    "            `early_stop_counter` (int)\n",
    "\n",
    "            `early_stopping_range` (int)\n",
    "\n",
    "            `epochs` (int)\n",
    "        \"\"\"\n",
    "        super().__init__(\n",
    "            semantic_encoder=semantic_encoder,\n",
    "            semantic_transformer=semantic_transformer,\n",
    "            acoustic_encoder_decoder=acoustic_encoder_decoder,\n",
    "            coarse_acoustic_transformer=coarse_acoustic_transformer,\n",
    "            train_dataloader=train_dataloader,\n",
    "            val_dataloader=val_dataloader,\n",
    "            test_dataloader=test_dataloader,\n",
    "            loss=loss,\n",
    "            optimizer=optimizer,\n",
    "            intervals=intervals,\n",
    "            save_path=save_path,\n",
    "            early_stop_counter=early_stop_counter,\n",
    "            early_stopping_range=early_stopping_range,\n",
    "            epochs=epochs,\n",
    "        )\n",
    "        self.generate_audio_len = generate_audio_len\n",
    "\n",
    "    def loss_generator(self, batch):\n",
    "\n",
    "        semantic_encode = self.semantic_encoder(batch)\n",
    "        print(semantic_encode.shape)\n",
    "        semantic_token = self.semantic_transformer.generate(semantic_encode, self.generate_audio_len * 50)\n",
    "       \n",
    "        coarse_acoustic_tokens, _, _ = self.acoustic_encoder_decoder.encode(batch)\n",
    "\n",
    "        conditioning = torch.cat((semantic_token, coarse_acoustic_tokens), dim=1)\n",
    "\n",
    "        output, target = self.coarse_acoustic_transformer.fit(conditioning)\n",
    "\n",
    "        loss = self.loss(output, target)\n",
    "        return loss\n",
    "\n",
    "    def train(self):\n",
    "        return self._train(self.coarse_acoustic_transformer)\n",
    "\n",
    "    def test(self):\n",
    "        return self._test(self.coarse_acoustic_transformer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Volumes/SSD-EXT/NN_project/models/models/SemanticTransformer.pth\n"
     ]
    }
   ],
   "source": [
    "print(MODEL_PATH / \"models\" / f\"{str(type(semantic_transformer_model).__name__)}.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8cf3ed951e444f8a42fb191c54ce984",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "282ebf6294c84b22bebc964573657718",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/34 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 149])\n",
      "Max length:  150\n",
      "torch.Size([4, 149])\n",
      "Max length:  150\n",
      "torch.Size([4, 149])\n",
      "Max length:  150\n",
      "torch.Size([4, 149])\n",
      "Max length:  150\n",
      "torch.Size([4, 149])\n",
      "Max length:  150\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 28\u001b[0m\n\u001b[1;32m      8\u001b[0m coarse_optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(\n\u001b[1;32m      9\u001b[0m                 coarse_acoustic_transformer_model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m\n\u001b[1;32m     10\u001b[0m             )\n\u001b[1;32m     11\u001b[0m coarse_acoustic_trainer \u001b[38;5;241m=\u001b[39m CoarseAcousticTrainer(\n\u001b[1;32m     12\u001b[0m                 semantic_encoder\u001b[38;5;241m=\u001b[39msemantic_encoder,\n\u001b[1;32m     13\u001b[0m                 semantic_transformer\u001b[38;5;241m=\u001b[39msemantic_transformer,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     26\u001b[0m                 epochs\u001b[38;5;241m=\u001b[39mEPOCHS,\n\u001b[1;32m     27\u001b[0m             )\n\u001b[0;32m---> 28\u001b[0m \u001b[43mcoarse_acoustic_trainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[11], line 89\u001b[0m, in \u001b[0;36mCoarseAcousticTrainer.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m---> 89\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcoarse_acoustic_transformer\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[6], line 109\u001b[0m, in \u001b[0;36mTrainer._train\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m    107\u001b[0m writer \u001b[38;5;241m=\u001b[39m SummaryWriter(Path(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_path) \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mruns\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mtype\u001b[39m(model)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m))\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepochs), total \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepochs, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 109\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    110\u001b[0m     validation_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validation_step(model)\n\u001b[1;32m    111\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSAVING CHECKPOINT...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[6], line 88\u001b[0m, in \u001b[0;36mTrainer._train_step\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m     86\u001b[0m     train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     87\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 88\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     89\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     90\u001b[0m train_loss \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_dataloader)\n",
      "File \u001b[0;32m~/Desktop/AudioLM/.venv/lib/python3.9/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/AudioLM/.venv/lib/python3.9/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "semantic_transformer = semantic_transformer_model\n",
    "state_dict = torch.load(\n",
    "            MODEL_PATH / \"models\" / f\"{str(type(semantic_transformer).__name__)}.pth\"\n",
    "        )\n",
    "semantic_transformer.load_state_dict(state_dict)\n",
    "coarse_acoustic_transformer_model = CoarseAcousticTransformer(num_heads=4,layers=2)\n",
    "coarse_loss = nn.CrossEntropyLoss()\n",
    "coarse_optimizer = torch.optim.Adam(\n",
    "                coarse_acoustic_transformer_model.parameters(), lr=0.001\n",
    "            )\n",
    "coarse_acoustic_trainer = CoarseAcousticTrainer(\n",
    "                semantic_encoder=semantic_encoder,\n",
    "                semantic_transformer=semantic_transformer,\n",
    "                acoustic_encoder_decoder=acoustic_encoder_decoder,\n",
    "                coarse_acoustic_transformer=coarse_acoustic_transformer_model,\n",
    "                train_dataloader=train_dataloader,\n",
    "                val_dataloader=val_dataloader,\n",
    "                test_dataloader=test_dataloader,\n",
    "                loss=coarse_loss,\n",
    "                optimizer=coarse_optimizer,\n",
    "                intervals=INTERVALS,\n",
    "                save_path=MODEL_PATH,\n",
    "                early_stop_counter=EARLY_STOP_COUNTER,\n",
    "                early_stopping_range=EARLY_STOPPING_RANGE,\n",
    "                generate_audio_len=3,\n",
    "                epochs=EPOCHS,\n",
    "            )\n",
    "coarse_acoustic_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "275\n"
     ]
    }
   ],
   "source": [
    "print(len(train_dataloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the sake of completness we implement the Fine acoustic trainer, even though it won't be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FineAcousticTrainer(Trainer):\n",
    "    \"\"\"Trainer class derived from `Trainer`.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        semantic_encoder: W2VHuBert,\n",
    "        semantic_transformer: SemanticTransformer,\n",
    "        acoustic_encoder_decoder: Encodec,\n",
    "        coarse_acoustic_transformer: CoarseAcousticTransformer,\n",
    "        fine_acoustic_transformer: FineAcousticTransformer,\n",
    "        train_dataloader: AudioDataLoader,\n",
    "        val_dataloader: AudioDataLoader,\n",
    "        test_dataloader: AudioDataLoader,\n",
    "        loss: nn.Module,\n",
    "        optimizer: torch.optim.Optimizer,\n",
    "        intervals: int,\n",
    "        save_path: Path,\n",
    "        early_stop_counter: int,\n",
    "        early_stopping_range: int,\n",
    "        generate_audio_len: int,\n",
    "        epochs: int,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Takes as input `semantic_encoder` and `semantic_transformer`.\n",
    "        They determine the `semantic_modelling`.\n",
    "\n",
    "        `semantic_encoder` must be trained ahead of time, this trainer only\n",
    "        trains `semantic_transformer`.\n",
    "\n",
    "        Args\n",
    "        ----\n",
    "            `semantic_encoder` (W2VHuBert)\n",
    "\n",
    "            `semantic_transformer` (TransformerDecoderOnly)\n",
    "\n",
    "            `train_dataloader` (AudioDataLoader)\n",
    "\n",
    "            `val_dataloader` (AudioDataLoader)\n",
    "\n",
    "            `test_dataloader` (AudioDataLoader)\n",
    "\n",
    "            `loss` (nn.Module)\n",
    "\n",
    "            `optimizer` (torch.optim.Optimizer)\n",
    "\n",
    "            `intervals` (int)\n",
    "\n",
    "            `save_path` (Path)\n",
    "\n",
    "            `early_stop_counter` (int)\n",
    "\n",
    "            `early_stopping_range` (int)\n",
    "\n",
    "            `epochs` (int)\n",
    "        \"\"\"\n",
    "        super().__init__(\n",
    "            semantic_encoder=semantic_encoder,\n",
    "            semantic_transformer=semantic_transformer,\n",
    "            acoustic_encoder_decoder=acoustic_encoder_decoder,\n",
    "            coarse_acoustic_transformer=coarse_acoustic_transformer,\n",
    "            fine_acoustic_transformer=fine_acoustic_transformer,\n",
    "            train_dataloader=train_dataloader,\n",
    "            val_dataloader=val_dataloader,\n",
    "            test_dataloader=test_dataloader,\n",
    "            loss=loss,\n",
    "            optimizer=optimizer,\n",
    "            intervals=intervals,\n",
    "            save_path=save_path,\n",
    "            early_stop_counter=early_stop_counter,\n",
    "            early_stopping_range=early_stopping_range,\n",
    "            epochs=epochs,\n",
    "        )\n",
    "        self.generate_audio_len = generate_audio_len\n",
    "\n",
    "    def loss_generator(self, batch):\n",
    "        semantic_encode = self.semantic_encoder(batch)\n",
    "        semantic_token = self.semantic_transformer.generate(semantic_encode, self.generate_audio_len * 50)\n",
    "\n",
    "        coarse_acoustic_tokens, fine_acoustic_tokens, _ = (\n",
    "            self.acoustic_encoder_decoder.encode(batch)\n",
    "        )\n",
    "        coarse_conditioning = torch.cat((semantic_token, coarse_acoustic_tokens), dim=1)\n",
    "        coarse_tokens = self.coarse_acoustic_transformer.generate(\n",
    "            coarse_conditioning, self.generate_audio_len * 75\n",
    "        )\n",
    "\n",
    "        output, target = self.fine_acoustic_transformer(\n",
    "            torch.cat((coarse_tokens, fine_acoustic_tokens), dim=1)\n",
    "        )\n",
    "        loss = self.loss(output, target)\n",
    "        return loss\n",
    "\n",
    "    def train(self):\n",
    "        return self._train(self.fine_acoustic_transformer)\n",
    "\n",
    "    def test(self):\n",
    "        return self._test(self.fine_acoustic_transformer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now define the full audiolm model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioLM:\n",
    "    def __init__(\n",
    "        self,\n",
    "        semantic_encoder: W2VHuBert,\n",
    "        semantic_transformer: SemanticTransformer,\n",
    "        acoustic_encoder_decoder: Encodec,\n",
    "        coarse_acoustic_transformer: CoarseAcousticTransformer,\n",
    "        fine_acoustic_transformer: FineAcousticTransformer,\n",
    "        # https://stackoverflow.com/a/53797072\n",
    "        *,\n",
    "        audio_len=1,\n",
    "        # We set Q' = 4 such that we predict the flattened tokens corresponding\n",
    "        # to the coarse 4 layers in the second stage.\n",
    "        n_coarse_quantizers=4,\n",
    "        # Not specified, but num quantizers must be a power of 2\n",
    "        # so this is the most reasonable combination.\n",
    "        n_fine_quantizers=4,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.semantic_encoder = semantic_encoder\n",
    "        for param in self.semantic_encoder.model.parameters():\n",
    "            param.requires_grad = False\n",
    "        self.acoustic_encoder_decoder = acoustic_encoder_decoder\n",
    "        for param in self.acoustic_encoder_decoder.model.parameters():\n",
    "            param.requires_grad = False\n",
    "        self.semantic_transformer = semantic_transformer\n",
    "        self.coarse_acoustic_transformer = coarse_acoustic_transformer\n",
    "        self.fine_acoustic_transformer = fine_acoustic_transformer\n",
    "        self.audio_len = audio_len\n",
    "        self.n_coarse_quantizers = n_coarse_quantizers\n",
    "        self.n_fine_quantizers = n_fine_quantizers\n",
    "\n",
    "    def generate(self, x: torch.Tensor, audio_len: int = 3):\n",
    "        \n",
    "        semantic_encode = self.semantic_encoder(x)\n",
    "        semantic_token = self.semantic_transformer.generate(\n",
    "            semantic_encode, audio_len * 50\n",
    "        )\n",
    "\n",
    "        coarse_acoustic_tokens, fine_acoustic_tokens, audio_scales = (\n",
    "            self.acoustic_encoder_decoder.encode(x)\n",
    "        )\n",
    "\n",
    "        coarse_conditioning = torch.cat((semantic_token, coarse_acoustic_tokens), dim=1)\n",
    "        coarse_tokens = self.coarse_acoustic_transformer.generate(\n",
    "            coarse_conditioning, audio_len * 75\n",
    "        )\n",
    "        \n",
    "        output = self.acoustic_encoder_decoder.decode(\n",
    "            coarse_tokens.unsqueeze(0).unsqueeze(0), [None]\n",
    "        )\n",
    "        return output[\"audio_values\"]\n",
    "    @staticmethod\n",
    "    def from_pretrained(\n",
    "        models_path: os.PathLike,\n",
    "        semantic_encoder : W2VHuBert,\n",
    "        acoustic_encoder_decoder : Encodec\n",
    "        ):\n",
    "        \n",
    "        semantic_transformer = semantic_transformer_model\n",
    "        state_dict = torch.load(\n",
    "            models_path / \"models\" / f\"{str(type(semantic_transformer).__name__)}.pth\"\n",
    "        )\n",
    "        semantic_transformer.load_state_dict(state_dict)\n",
    "        \n",
    "        coarse_acoustic_transformer = coarse_acoustic_transformer_model\n",
    "        state_dict = torch.load(\n",
    "            models_path\n",
    "            / \"models\"\n",
    "            / f\"{str(type(coarse_acoustic_transformer).__name__)}.pth\"\n",
    "        )\n",
    "        coarse_acoustic_transformer.load_state_dict(state_dict)\n",
    "        return AudioLM(\n",
    "            semantic_encoder=semantic_encoder,\n",
    "            semantic_transformer=semantic_transformer,\n",
    "            acoustic_encoder_decoder=acoustic_encoder_decoder,\n",
    "            coarse_acoustic_transformer=coarse_acoustic_transformer,\n",
    "            fine_acoustic_transformer=None,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now create a function for the instantation of the model assuming that the transformers are trained ahead of time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Volumes/SSD-EXT/NN_project/models/models/CoarseAcousticTransformer.pth'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m audiolm \u001b[38;5;241m=\u001b[39m \u001b[43mAudioLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mMODEL_PATH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msemantic_encoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43macoustic_encoder_decoder\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[34], line 67\u001b[0m, in \u001b[0;36mAudioLM.from_pretrained\u001b[0;34m(models_path, semantic_encoder, acoustic_encoder_decoder)\u001b[0m\n\u001b[1;32m     64\u001b[0m semantic_transformer\u001b[38;5;241m.\u001b[39mload_state_dict(state_dict)\n\u001b[1;32m     66\u001b[0m coarse_acoustic_transformer \u001b[38;5;241m=\u001b[39m coarse_acoustic_transformer_model\n\u001b[0;32m---> 67\u001b[0m state_dict \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodels_path\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodels\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m     70\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcoarse_acoustic_transformer\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.pth\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m     71\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     72\u001b[0m coarse_acoustic_transformer\u001b[38;5;241m.\u001b[39mload_state_dict(state_dict)\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m AudioLM(\n\u001b[1;32m     74\u001b[0m     semantic_encoder\u001b[38;5;241m=\u001b[39msemantic_encoder,\n\u001b[1;32m     75\u001b[0m     semantic_transformer\u001b[38;5;241m=\u001b[39msemantic_transformer,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     78\u001b[0m     fine_acoustic_transformer\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     79\u001b[0m )\n",
      "File \u001b[0;32m~/Desktop/AudioLM/.venv/lib/python3.9/site-packages/torch/serialization.py:998\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m    995\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    996\u001b[0m     pickle_load_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 998\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[1;32m    999\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m   1000\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m   1001\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m   1002\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m   1003\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[0;32m~/Desktop/AudioLM/.venv/lib/python3.9/site-packages/torch/serialization.py:445\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    443\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[1;32m    444\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 445\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    446\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    447\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[0;32m~/Desktop/AudioLM/.venv/lib/python3.9/site-packages/torch/serialization.py:426\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    425\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[0;32m--> 426\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Volumes/SSD-EXT/NN_project/models/models/CoarseAcousticTransformer.pth'"
     ]
    }
   ],
   "source": [
    "audiolm = AudioLM.from_pretrained(MODEL_PATH, semantic_encoder, acoustic_encoder_decoder)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
